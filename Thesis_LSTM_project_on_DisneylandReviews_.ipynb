{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4fE1W8OydP"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "n24yBUOo6PYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/MyDrive/datasets/master_thesis/DisneylandReviews.csv.zip\" \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8jNrVv0V1I",
        "outputId": "d9588756-aa4d-4cec-c900-623f397836d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'DisneylandReviews.csv.zip'\n",
        "\n",
        "extract_to_path = ''\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)"
      ],
      "metadata": {
        "id": "PSy5CxDfMDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"DisneylandReviews.csv\",encoding='ISO-8859-1')\n",
        "train_text_list=[text[:1500] for text in train_df['Review_Text']]\n",
        "train_target_list=[text for text in train_df['Rating']]\n",
        "train_target_list = [1 if x > 3 else 0 for x in train_target_list]\n",
        "train_text_negative=[]\n",
        "train_text_positive=[]\n",
        "train_target_negative=[]\n",
        "train_target_positive=[]\n",
        "for idx, y in enumerate(train_target_list):\n",
        "  if y==0:\n",
        "    train_text_negative.append(train_text_list[idx])\n",
        "    train_target_negative.append(train_target_list[idx])\n",
        "  else:\n",
        "    train_text_positive.append(train_text_list[idx])\n",
        "    train_target_positive.append(train_target_list[idx])\n",
        "\n",
        "\n",
        "import random\n",
        "negative_indices = random.sample(range(len(train_text_negative)), 700)\n",
        "positive_indices = random.sample(range(len(train_text_positive)), 700)\n",
        "\n",
        "train_text_list=[train_text_negative[i] for i in negative_indices[:500]]+[train_text_positive[i] for i in positive_indices[:500]]\n",
        "train_target_list=[train_target_negative[i] for i in negative_indices[:500]]+[train_target_positive[i] for i in positive_indices[:500]]\n",
        "\n",
        "valid_text_list=[train_text_negative[i] for i in negative_indices[500:600]]+[train_text_positive[i] for i in positive_indices[500:600]]\n",
        "valid_target_list=[train_target_negative[i] for i in negative_indices[500:600]]+[train_target_positive[i] for i in positive_indices[500:600]]\n",
        "\n",
        "test_text_list=[train_text_negative[i] for i in negative_indices[600:]]+[train_text_positive[i] for i in positive_indices[600:]]\n",
        "test_target_list=[train_target_negative[i] for i in negative_indices[600:]]+[train_target_positive[i] for i in positive_indices[600:]]\n",
        "\n",
        "text_list=[text for text in train_text_list]+[text for text in valid_text_list]+[text for text in test_text_list]\n",
        "\n",
        "train_text_array=np.array(train_text_list)\n",
        "train_target_array=np.array(train_target_list)\n",
        "\n",
        "valid_text_array=np.array(valid_text_list)\n",
        "valid_target_array=np.array(valid_target_list)\n",
        "\n",
        "test_text_array=np.array(test_text_list)\n",
        "test_target_array=np.array(test_target_list)\n",
        "\n",
        "BUFFER_SIZE = len(train_text_list)\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "7yhFJpeBNsqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mh1RBDZPBpq"
      },
      "outputs": [],
      "source": [
        "def clean_and_format_text(input_text):\n",
        "  lower_text = tf.strings.lower(input_text)\n",
        "  cleaned_text = tf.strings.regex_replace(lower_text, '[^ a-z.?!,¿]', '')\n",
        "  spaced_text = tf.strings.regex_replace(cleaned_text, '[.?!,¿]', r' \\0 ')\n",
        "  final_text=tf.strings.strip(spaced_text)\n",
        "  return final_text\n",
        "\n",
        "\n",
        "maximum_vocab_size = 20000\n",
        "\n",
        "text_vectorizer = preprocessing.TextVectorization(\n",
        "    standardize=clean_and_format_text,\n",
        "    max_tokens=maximum_vocab_size\n",
        ")\n",
        "text_vectorizer.adapt(text_list)\n",
        "def pair_tokenizer(input_text, target_text):\n",
        "    processed_input = text_vectorizer(input_text)\n",
        "    return processed_input, target_text\n",
        "\n",
        "def prepare_dataset_batches(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(pair_tokenizer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46N1ttkVJgZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text_array, train_target_array))\n",
        "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_text_array, valid_target_array))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((test_text_array, test_target_array))\n",
        "\n",
        "train_batches=prepare_dataset_batches(train_dataset)\n",
        "valid_batches=prepare_dataset_batches(valid_dataset)\n",
        "test_batches=prepare_dataset_batches(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the Model"
      ],
      "metadata": {
        "id": "yROfOJasIz0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byVijLXYVLl9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHGp54ZVNRZ"
      },
      "outputs": [],
      "source": [
        "class TheSelfAttention(keras.layers.Layer):\n",
        "  def __init__(self,embed_size,heads,**kwargs):\n",
        "    super(TheSelfAttention,self).__init__(**kwargs)\n",
        "\n",
        "    self.embedding_dim = embed_size\n",
        "    self.num_heads = heads\n",
        "    self.depth = embed_size//heads\n",
        "\n",
        "    assert self.depth * self.num_heads == self.embedding_dim\n",
        "\n",
        "    self.dense_value = keras.layers.Dense(self.depth, name=\"value\")\n",
        "    self.dense_key = keras.layers.Dense(self.depth, name=\"key\")\n",
        "    self.dense_query = keras.layers.Dense(self.depth, name=\"query\")\n",
        "    self.final_dense = keras.layers.Dense(self.embedding_dim, name=\"output\")\n",
        "\n",
        "  def call(self,values,keys,queries,mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    def split_heads(x):\n",
        "        return tf.reshape(x, (batch_size, tf.shape(x)[1], self.num_heads, self.depth))\n",
        "\n",
        "    values = self.dense_value(split_heads(values))\n",
        "    keys = self.dense_key(split_heads(keys))\n",
        "    queries = self.dense_query(split_heads(queries))\n",
        "\n",
        "    attention_scores = tf.einsum(\"bnhd,bmhd->bhnm\", queries, keys)\n",
        "\n",
        "    attention_weights=tf.nn.softmax(attention_scores/(self.depth**(1/2)),axis=3)\n",
        "    context_layer=tf.reshape(tf.einsum(\"nhql,nlhd->nqhd\", attention_weights,values),(batch_size,tf.shape(queries)[1],self.num_heads*self.depth))\n",
        "    attended_output = self.final_dense(context_layer)\n",
        "    return attended_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzcCvdzHVPD3"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "  def __init__(self,embedding_dim, num_heads, dropout_rate, forward_expansion, **kwargs):\n",
        "    super(TransformerBlock,self).__init__(**kwargs)\n",
        "    self.self_attention=TheSelfAttention(embedding_dim, num_heads)\n",
        "    self.norm1=keras.layers.LayerNormalization()\n",
        "    self.norm2=keras.layers.LayerNormalization()\n",
        "\n",
        "    self.feed_forward=tf.keras.Sequential([\n",
        "        keras.layers.Dense(forward_expansion * embedding_dim,input_shape=(None,embed_size)),\n",
        "        keras.layers.Activation(\"relu\"),\n",
        "        keras.layers.Dense(embedding_dim)\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    self.dropout=keras.layers.Dropout(dropout)\n",
        "  def call(self,values,keys,queries,mask,training):\n",
        "    attention_output=self.self_attention(values,keys,queries,mask)\n",
        "\n",
        "    out1=self.dropout(self.norm1(attention_output+queries))\n",
        "    forward_output=self.feed_forward(out1)\n",
        "    out2=self.dropout(self.norm2(forward_output+out1),training=training)\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtoKfz3MVRCL"
      },
      "outputs": [],
      "source": [
        "class Model(keras.layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      embedding_dim,\n",
        "      num_layers,\n",
        "      num_heads,\n",
        "      forward_expansion,\n",
        "      dropout_rate,\n",
        "      max_length,\n",
        "      **kwargs\n",
        "    ):\n",
        "    super(Model,self).__init__(**kwargs)\n",
        "    self.embed_size=embed_size\n",
        "    self.token_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.position_embedding = keras.layers.Embedding(max_length, output_dim=embedding_dim)\n",
        "    self.transformer_layers = [TransformerBlock(embedding_dim, num_heads, dropout_rate, forward_expansion) for _ in range(num_layers)]\n",
        "    self.dropout=keras.layers.Dropout(dropout_rate)\n",
        "    self.final_layer=tf.keras.layers.LSTM(2,activation='softmax')\n",
        "\n",
        "  def call(self,inputs,mask,training):\n",
        "    batch_size=tf.shape(inputs)[0]\n",
        "    seq_len=tf.shape(inputs)[1]\n",
        "\n",
        "    positions=tf.range(0,seq_len)\n",
        "    positions=tf.reshape(positions,(1,seq_len))\n",
        "    positions=tf.tile(positions,[batch_size,1])\n",
        "\n",
        "    positions = self.position_embedding(positions)\n",
        "    x = self.token_embedding(inputs) + positions\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for layer in self.transformer_layers:\n",
        "      x=layer(x,x,x,mask,training=training)\n",
        "    x = self.dropout(x, training=training)\n",
        "    output = self.final_layer(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and Training"
      ],
      "metadata": {
        "id": "KnO-TI-aI3ZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudIcRIrVS7T"
      },
      "outputs": [],
      "source": [
        "num_epochs=30\n",
        "learning_rate=4e-5\n",
        "src_vocab_size=maximum_vocab_size\n",
        "embed_size=512\n",
        "heads=8\n",
        "num_encoder_layers=4\n",
        "dropout=0.1\n",
        "max_length=100\n",
        "forward_expansion=2\n",
        "\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate)\n",
        "loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_accuracies=[]\n",
        "valid_accuracies=[]\n",
        "test_accuracies=[]\n",
        "\n",
        "train_f1s=[]\n",
        "valid_f1s=[]\n",
        "test_f1s=[]\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.cast(tf.shape(accuracies)[0],dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "train_f1=tf.keras.metrics.Mean(name='train_f1')\n",
        "\n",
        "valid_loss=tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy=tf.keras.metrics.Mean(name='valid_accuracy')\n",
        "valid_f1=tf.keras.metrics.Mean(name='valid_f1')\n",
        "\n",
        "test_loss=tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy=tf.keras.metrics.Mean(name='test_accuracy')\n",
        "test_f1=tf.keras.metrics.Mean(name='test_f1')\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_function(real, pred):\n",
        "\n",
        "    predicted_classes = tf.argmax(pred, axis=1)\n",
        "\n",
        "    TP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 0), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 0)), dtype=tf.float32))\n",
        "\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "\n",
        "    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n",
        "    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n",
        "\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "\n",
        "    f1_score = tf.where(tf.math.is_nan(f1_score), tf.zeros_like(f1_score), f1_score)\n",
        "\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "KiJeDEnhptJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(\n",
        "          src_vocab_size,\n",
        "          embed_size,\n",
        "          num_encoder_layers,\n",
        "          heads,\n",
        "          forward_expansion,\n",
        "          dropout,\n",
        "          max_length\n",
        "      )\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp_data,target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output=model(inp_data,None,True)\n",
        "    loss=loss_object(target,output)\n",
        "  gradients=tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(target,output))\n",
        "  train_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(accuracy_function(target,output))\n",
        "  valid_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def test_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  test_loss(loss)\n",
        "  test_accuracy(accuracy_function(target,output))\n",
        "  test_f1(f1_score_function(target,output))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  train_f1.reset_states()\n",
        "\n",
        "  valid_accuracy.reset_states()\n",
        "  valid_f1.reset_states()\n",
        "  valid_loss.reset_states()\n",
        "\n",
        "  test_accuracy.reset_states()\n",
        "  test_f1.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    train_step(inp, tar)\n",
        "  for inp,tar in valid_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    valid_step(inp,tar)\n",
        "\n",
        "  for inp,tar in test_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    test_step(inp,tar)\n",
        "\n",
        "  print(f'Loss {train_loss.result():.4f} train_f1 {train_f1.result():.4f} Accuracy {train_accuracy.result():.4f}\\\n",
        "   valid_Loss {valid_loss.result():.4f} valid_Accuracy {valid_accuracy.result():.4f} valid_f1 {valid_f1.result():.4f}  test_Loss {test_loss.result():.4f} test_Accuracy {test_accuracy.result():.4f} test_f1 {test_f1.result():.4f}')\n",
        "\n",
        "  train_accuracies.append(train_accuracy.result())\n",
        "  valid_accuracies.append(valid_accuracy.result())\n",
        "  test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "  train_f1s.append(train_f1.result())\n",
        "  valid_f1s.append(valid_f1.result())\n",
        "  test_f1s.append(test_f1.result())\n"
      ],
      "metadata": {
        "id": "ZgyjgFh9puJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7dbc0b-ed6d-480c-96ad-b04ca4dbc202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.7065 train_f1 0.5278 Accuracy 0.5347   valid_Loss 0.6754 valid_Accuracy 0.6202 valid_f1 0.6534  test_Loss 0.6855 test_Accuracy 0.6202 test_f1 0.6303\n",
            "Loss 0.6835 train_f1 0.5295 Accuracy 0.5734   valid_Loss 0.6456 valid_Accuracy 0.6058 valid_f1 0.6506  test_Loss 0.6711 test_Accuracy 0.6106 test_f1 0.6474\n",
            "Loss 0.6776 train_f1 0.5804 Accuracy 0.5823   valid_Loss 0.6577 valid_Accuracy 0.5913 valid_f1 0.6352  test_Loss 0.6685 test_Accuracy 0.5817 test_f1 0.6222\n",
            "Loss 0.6783 train_f1 0.4983 Accuracy 0.5923   valid_Loss 0.6667 valid_Accuracy 0.5865 valid_f1 0.6601  test_Loss 0.6816 test_Accuracy 0.5769 test_f1 0.6542\n",
            "Loss 0.6738 train_f1 0.5817 Accuracy 0.5933   valid_Loss 0.6602 valid_Accuracy 0.6010 valid_f1 0.6768  test_Loss 0.6803 test_Accuracy 0.5721 test_f1 0.6413\n",
            "Loss 0.6645 train_f1 0.6210 Accuracy 0.6042   valid_Loss 0.6706 valid_Accuracy 0.6010 valid_f1 0.6011  test_Loss 0.6667 test_Accuracy 0.6202 test_f1 0.6094\n",
            "Loss 0.6696 train_f1 0.5763 Accuracy 0.5893   valid_Loss 0.6638 valid_Accuracy 0.6346 valid_f1 0.6142  test_Loss 0.6702 test_Accuracy 0.6202 test_f1 0.5889\n",
            "Loss 0.6580 train_f1 0.6138 Accuracy 0.6200   valid_Loss 0.6500 valid_Accuracy 0.6250 valid_f1 0.5834  test_Loss 0.6587 test_Accuracy 0.6394 test_f1 0.6146\n",
            "Loss 0.6456 train_f1 0.5656 Accuracy 0.6171   valid_Loss 0.6536 valid_Accuracy 0.6106 valid_f1 0.6014  test_Loss 0.6489 test_Accuracy 0.6442 test_f1 0.6258\n",
            "Loss 0.6481 train_f1 0.6116 Accuracy 0.6042   valid_Loss 0.6579 valid_Accuracy 0.6250 valid_f1 0.6469  test_Loss 0.6402 test_Accuracy 0.6394 test_f1 0.6619\n",
            "Loss 0.6206 train_f1 0.6389 Accuracy 0.6498   valid_Loss 0.6474 valid_Accuracy 0.6346 valid_f1 0.6087  test_Loss 0.6134 test_Accuracy 0.6827 test_f1 0.6839\n",
            "Loss 0.6747 train_f1 0.6654 Accuracy 0.5308   valid_Loss 0.6929 valid_Accuracy 0.4904 valid_f1 0.6483  test_Loss 0.6926 test_Accuracy 0.5048 test_f1 0.6648\n",
            "Loss 0.6748 train_f1 0.6696 Accuracy 0.5258   valid_Loss 0.6831 valid_Accuracy 0.6298 valid_f1 0.7058  test_Loss 0.8959 test_Accuracy 0.5962 test_f1 0.6505\n",
            "Loss 0.7036 train_f1 0.6612 Accuracy 0.5079   valid_Loss 0.6934 valid_Accuracy 0.5048 valid_f1 0.6621  test_Loss 0.6927 test_Accuracy 0.5144 test_f1 0.6630\n",
            "Loss 0.6928 train_f1 0.6497 Accuracy 0.4940   valid_Loss 0.6915 valid_Accuracy 0.5000 valid_f1 0.6562  test_Loss 0.6911 test_Accuracy 0.5000 test_f1 0.6582\n",
            "Loss 0.6913 train_f1 0.6590 Accuracy 0.5030   valid_Loss 0.6876 valid_Accuracy 0.5048 valid_f1 0.6656  test_Loss 0.6875 test_Accuracy 0.4952 test_f1 0.6573\n",
            "Loss 0.6864 train_f1 0.6568 Accuracy 0.5000   valid_Loss 0.6816 valid_Accuracy 0.4904 valid_f1 0.6491  test_Loss 0.6760 test_Accuracy 0.4952 test_f1 0.6520\n",
            "Loss 0.6778 train_f1 0.6625 Accuracy 0.5050   valid_Loss 0.6875 valid_Accuracy 0.5192 valid_f1 0.6650  test_Loss 0.6857 test_Accuracy 0.5337 test_f1 0.6773\n",
            "Loss 0.6640 train_f1 0.6611 Accuracy 0.5139   valid_Loss 0.6597 valid_Accuracy 0.5048 valid_f1 0.6575  test_Loss 0.6434 test_Accuracy 0.5192 test_f1 0.6557\n",
            "Loss 0.6163 train_f1 0.7157 Accuracy 0.6706   valid_Loss 0.6164 valid_Accuracy 0.6683 valid_f1 0.5865  test_Loss 0.6336 test_Accuracy 0.6683 test_f1 0.5718\n",
            "Loss 0.4466 train_f1 0.8050 Accuracy 0.8194   valid_Loss 0.5451 valid_Accuracy 0.7115 valid_f1 0.7359  test_Loss 0.5152 test_Accuracy 0.7260 test_f1 0.7565\n",
            "Loss 0.4084 train_f1 0.8454 Accuracy 0.8363   valid_Loss 1.1124 valid_Accuracy 0.6250 valid_f1 0.4332  test_Loss 0.9628 test_Accuracy 0.6635 test_f1 0.5273\n",
            "Loss 0.3304 train_f1 0.8720 Accuracy 0.8780   valid_Loss 0.5199 valid_Accuracy 0.7163 valid_f1 0.6728  test_Loss 0.4941 test_Accuracy 0.7596 test_f1 0.7613\n",
            "Loss 0.2185 train_f1 0.9286 Accuracy 0.9306   valid_Loss 0.9067 valid_Accuracy 0.7115 valid_f1 0.6396  test_Loss 0.6563 test_Accuracy 0.7740 test_f1 0.7546\n",
            "Loss 0.1629 train_f1 0.9474 Accuracy 0.9464   valid_Loss 0.7915 valid_Accuracy 0.7356 valid_f1 0.7459  test_Loss 0.7401 test_Accuracy 0.7740 test_f1 0.7946\n",
            "Loss 0.1140 train_f1 0.9573 Accuracy 0.9583   valid_Loss 1.2922 valid_Accuracy 0.7212 valid_f1 0.6534  test_Loss 0.8896 test_Accuracy 0.7644 test_f1 0.7192\n",
            "Loss 0.0987 train_f1 0.9642 Accuracy 0.9623   valid_Loss 2.1420 valid_Accuracy 0.6490 valid_f1 0.4846  test_Loss 1.7447 test_Accuracy 0.6538 test_f1 0.4635\n",
            "Loss 0.0967 train_f1 0.9564 Accuracy 0.9613   valid_Loss 1.2240 valid_Accuracy 0.6971 valid_f1 0.6562  test_Loss 0.8244 test_Accuracy 0.7548 test_f1 0.7187\n",
            "Loss 0.0640 train_f1 0.9727 Accuracy 0.9742   valid_Loss 1.3950 valid_Accuracy 0.7067 valid_f1 0.6759  test_Loss 1.0199 test_Accuracy 0.7596 test_f1 0.7310\n",
            "Loss 0.0419 train_f1 0.9896 Accuracy 0.9901   valid_Loss 1.4901 valid_Accuracy 0.7115 valid_f1 0.6372  test_Loss 1.0648 test_Accuracy 0.7692 test_f1 0.7400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save result"
      ],
      "metadata": {
        "id": "jVPyJN6nJxTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Train Accuracies': [i.numpy() for i in train_accuracies],\n",
        "    'Valid Accuracies': [i.numpy() for i in valid_accuracies],\n",
        "    'Test Accuracies': [i.numpy() for i in test_accuracies],\n",
        "    'Train F1 Scores': [i.numpy() for i in train_f1s],\n",
        "    'Valid F1 Scores': [i.numpy() for i in valid_f1s],\n",
        "    'Test F1 Scores': [i.numpy() for i in test_f1s]\n",
        "})\n",
        "\n",
        "excel_file_path = 'LSTM_Disneyland5.xlsx'\n",
        "df.to_excel(excel_file_path, engine='openpyxl')\n",
        "\n",
        "!cp -r \"/content/LSTM_Disneyland5.xlsx\" \"/content/drive/MyDrive/datasets/master_thesis/record/\""
      ],
      "metadata": {
        "id": "anqwptlD9k4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "highest_value = max(valid_accuracies)\n",
        "\n",
        "\n",
        "index_of_highest = valid_accuracies.index(highest_value)\n",
        "\n",
        "print(test_accuracies[index_of_highest], test_f1s[index_of_highest])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgT-HdgC9Wa",
        "outputId": "afb8025a-27ec-4a42-e983-4d40b218335a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.77403843, shape=(), dtype=float32) tf.Tensor(0.7945787, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PChxLeMJIhFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}