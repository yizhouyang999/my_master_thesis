{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4fE1W8OydP"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "n24yBUOo6PYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/MyDrive/datasets/master_thesis/winemag-data-130k-v2.csv.zip\" \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8jNrVv0V1I",
        "outputId": "6c2ae24c-4697-4ea9-bd55-57a424af6911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'winemag-data-130k-v2.csv.zip'\n",
        "\n",
        "extract_to_path = ''\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)"
      ],
      "metadata": {
        "id": "PSy5CxDfMDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"winemag-data-130k-v2.csv\",encoding='ISO-8859-1')\n",
        "train_text_list=[text[:1500] for text in train_df['description']]\n",
        "train_target_list=[text for text in train_df['points']]\n",
        "train_target_list = [1 if x > 90 else 0 for x in train_target_list]\n",
        "train_text_negative=[]\n",
        "train_text_positive=[]\n",
        "train_target_negative=[]\n",
        "train_target_positive=[]\n",
        "for idx, y in enumerate(train_target_list):\n",
        "  if y==0:\n",
        "    train_text_negative.append(train_text_list[idx])\n",
        "    train_target_negative.append(train_target_list[idx])\n",
        "  else:\n",
        "    train_text_positive.append(train_text_list[idx])\n",
        "    train_target_positive.append(train_target_list[idx])\n",
        "\n",
        "\n",
        "import random\n",
        "negative_indices = random.sample(range(len(train_text_negative)), 700)\n",
        "positive_indices = random.sample(range(len(train_text_positive)), 700)\n",
        "\n",
        "train_text_list=[train_text_negative[i] for i in negative_indices[:500]]+[train_text_positive[i] for i in positive_indices[:500]]\n",
        "train_target_list=[train_target_negative[i] for i in negative_indices[:500]]+[train_target_positive[i] for i in positive_indices[:500]]\n",
        "\n",
        "valid_text_list=[train_text_negative[i] for i in negative_indices[500:600]]+[train_text_positive[i] for i in positive_indices[500:600]]\n",
        "valid_target_list=[train_target_negative[i] for i in negative_indices[500:600]]+[train_target_positive[i] for i in positive_indices[500:600]]\n",
        "\n",
        "test_text_list=[train_text_negative[i] for i in negative_indices[600:]]+[train_text_positive[i] for i in positive_indices[600:]]\n",
        "test_target_list=[train_target_negative[i] for i in negative_indices[600:]]+[train_target_positive[i] for i in positive_indices[600:]]\n",
        "\n",
        "text_list=[text for text in train_text_list]+[text for text in valid_text_list]+[text for text in test_text_list]\n",
        "\n",
        "train_text_array=np.array(train_text_list)\n",
        "train_target_array=np.array(train_target_list)\n",
        "\n",
        "valid_text_array=np.array(valid_text_list)\n",
        "valid_target_array=np.array(valid_target_list)\n",
        "\n",
        "test_text_array=np.array(test_text_list)\n",
        "test_target_array=np.array(test_target_list)\n",
        "\n",
        "BUFFER_SIZE = len(train_text_list)\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "7yhFJpeBNsqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mh1RBDZPBpq"
      },
      "outputs": [],
      "source": [
        "def clean_and_format_text(input_text):\n",
        "  lower_text = tf.strings.lower(input_text)\n",
        "  cleaned_text = tf.strings.regex_replace(lower_text, '[^ a-z.?!,¿]', '')\n",
        "  spaced_text = tf.strings.regex_replace(cleaned_text, '[.?!,¿]', r' \\0 ')\n",
        "  final_text=tf.strings.strip(spaced_text)\n",
        "  return final_text\n",
        "\n",
        "\n",
        "maximum_vocab_size = 20000\n",
        "\n",
        "text_vectorizer = preprocessing.TextVectorization(\n",
        "    standardize=clean_and_format_text,\n",
        "    max_tokens=maximum_vocab_size\n",
        ")\n",
        "text_vectorizer.adapt(text_list)\n",
        "def pair_tokenizer(input_text, target_text):\n",
        "    processed_input = text_vectorizer(input_text)\n",
        "    return processed_input, target_text\n",
        "\n",
        "def prepare_dataset_batches(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(pair_tokenizer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46N1ttkVJgZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text_array, train_target_array))\n",
        "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_text_array, valid_target_array))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((test_text_array, test_target_array))\n",
        "\n",
        "train_batches=prepare_dataset_batches(train_dataset)\n",
        "valid_batches=prepare_dataset_batches(valid_dataset)\n",
        "test_batches=prepare_dataset_batches(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the Model"
      ],
      "metadata": {
        "id": "ovzjQEZYHgiP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byVijLXYVLl9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHGp54ZVNRZ"
      },
      "outputs": [],
      "source": [
        "class TheSelfAttention(keras.layers.Layer):\n",
        "  def __init__(self,embed_size,heads,**kwargs):\n",
        "    super(TheSelfAttention,self).__init__(**kwargs)\n",
        "\n",
        "    self.embedding_dim = embed_size\n",
        "    self.num_heads = heads\n",
        "    self.depth = embed_size//heads\n",
        "\n",
        "    assert self.depth * self.num_heads == self.embedding_dim\n",
        "\n",
        "    self.dense_value = keras.layers.Dense(self.depth, name=\"value\")\n",
        "    self.dense_key = keras.layers.Dense(self.depth, name=\"key\")\n",
        "    self.dense_query = keras.layers.Dense(self.depth, name=\"query\")\n",
        "    self.final_dense = keras.layers.Dense(self.embedding_dim, name=\"output\")\n",
        "\n",
        "  def call(self,values,keys,queries,mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    def split_heads(x):\n",
        "        return tf.reshape(x, (batch_size, tf.shape(x)[1], self.num_heads, self.depth))\n",
        "\n",
        "    values = self.dense_value(split_heads(values))\n",
        "    keys = self.dense_key(split_heads(keys))\n",
        "    queries = self.dense_query(split_heads(queries))\n",
        "\n",
        "    attention_scores = tf.einsum(\"bnhd,bmhd->bhnm\", queries, keys)\n",
        "\n",
        "    attention_weights=tf.nn.softmax(attention_scores/(self.depth**(1/2)),axis=3)\n",
        "    context_layer=tf.reshape(tf.einsum(\"nhql,nlhd->nqhd\", attention_weights,values),(batch_size,tf.shape(queries)[1],self.num_heads*self.depth))\n",
        "    attended_output = self.final_dense(context_layer)\n",
        "    return attended_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzcCvdzHVPD3"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "  def __init__(self,embedding_dim, num_heads, dropout_rate, forward_expansion, **kwargs):\n",
        "    super(TransformerBlock,self).__init__(**kwargs)\n",
        "    self.self_attention=TheSelfAttention(embedding_dim, num_heads)\n",
        "    self.norm1=keras.layers.LayerNormalization()\n",
        "    self.norm2=keras.layers.LayerNormalization()\n",
        "\n",
        "    self.feed_forward=tf.keras.Sequential([\n",
        "        keras.layers.Dense(forward_expansion * embedding_dim,input_shape=(None,embed_size)),\n",
        "        keras.layers.Activation(\"relu\"),\n",
        "        keras.layers.Dense(embedding_dim)\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    self.dropout=keras.layers.Dropout(dropout)\n",
        "  def call(self,values,keys,queries,mask,training):\n",
        "    attention_output=self.self_attention(values,keys,queries,mask)\n",
        "\n",
        "    out1=self.dropout(self.norm1(attention_output+queries))\n",
        "    forward_output=self.feed_forward(out1)\n",
        "    out2=self.dropout(self.norm2(forward_output+out1),training=training)\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtoKfz3MVRCL"
      },
      "outputs": [],
      "source": [
        "class Model(keras.layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      embedding_dim,\n",
        "      num_layers,\n",
        "      num_heads,\n",
        "      forward_expansion,\n",
        "      dropout_rate,\n",
        "      max_length,\n",
        "      **kwargs\n",
        "    ):\n",
        "    super(Model,self).__init__(**kwargs)\n",
        "    self.embed_size=embed_size\n",
        "    self.token_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.position_embedding = keras.layers.Embedding(max_length, output_dim=embedding_dim)\n",
        "    self.transformer_layers = [TransformerBlock(embedding_dim, num_heads, dropout_rate, forward_expansion) for _ in range(num_layers)]\n",
        "    self.dropout=keras.layers.Dropout(dropout_rate)\n",
        "    self.final_layer=tf.keras.layers.LSTM(2,activation='softmax')\n",
        "\n",
        "  def call(self,inputs,mask,training):\n",
        "    batch_size=tf.shape(inputs)[0]\n",
        "    seq_len=tf.shape(inputs)[1]\n",
        "\n",
        "    positions=tf.range(0,seq_len)\n",
        "    positions=tf.reshape(positions,(1,seq_len))\n",
        "    positions=tf.tile(positions,[batch_size,1])\n",
        "\n",
        "    positions = self.position_embedding(positions)\n",
        "    x = self.token_embedding(inputs) + positions\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for layer in self.transformer_layers:\n",
        "      x=layer(x,x,x,mask,training=training)\n",
        "    x = self.dropout(x, training=training)\n",
        "    output = self.final_layer(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and training"
      ],
      "metadata": {
        "id": "u1kDdrh1Hj3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudIcRIrVS7T"
      },
      "outputs": [],
      "source": [
        "num_epochs=30\n",
        "learning_rate=4e-5\n",
        "src_vocab_size=maximum_vocab_size\n",
        "embed_size=512\n",
        "heads=8\n",
        "num_encoder_layers=4\n",
        "dropout=0.1\n",
        "max_length=100\n",
        "forward_expansion=2\n",
        "\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate)\n",
        "loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_accuracies=[]\n",
        "valid_accuracies=[]\n",
        "test_accuracies=[]\n",
        "\n",
        "train_f1s=[]\n",
        "valid_f1s=[]\n",
        "test_f1s=[]\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.cast(tf.shape(accuracies)[0],dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "train_f1=tf.keras.metrics.Mean(name='train_f1')\n",
        "\n",
        "valid_loss=tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy=tf.keras.metrics.Mean(name='valid_accuracy')\n",
        "valid_f1=tf.keras.metrics.Mean(name='valid_f1')\n",
        "\n",
        "test_loss=tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy=tf.keras.metrics.Mean(name='test_accuracy')\n",
        "test_f1=tf.keras.metrics.Mean(name='test_f1')\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_function(real, pred):\n",
        "\n",
        "    predicted_classes = tf.argmax(pred, axis=1)\n",
        "\n",
        "    TP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 0), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 0)), dtype=tf.float32))\n",
        "\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "\n",
        "    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n",
        "    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n",
        "\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "\n",
        "    f1_score = tf.where(tf.math.is_nan(f1_score), tf.zeros_like(f1_score), f1_score)\n",
        "\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "KiJeDEnhptJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(\n",
        "          src_vocab_size,\n",
        "          embed_size,\n",
        "          num_encoder_layers,\n",
        "          heads,\n",
        "          forward_expansion,\n",
        "          dropout,\n",
        "          max_length\n",
        "      )\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp_data,target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output=model(inp_data,None,True)\n",
        "    loss=loss_object(target,output)\n",
        "  gradients=tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(target,output))\n",
        "  train_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(accuracy_function(target,output))\n",
        "  valid_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def test_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  test_loss(loss)\n",
        "  test_accuracy(accuracy_function(target,output))\n",
        "  test_f1(f1_score_function(target,output))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  train_f1.reset_states()\n",
        "\n",
        "  valid_accuracy.reset_states()\n",
        "  valid_f1.reset_states()\n",
        "  valid_loss.reset_states()\n",
        "\n",
        "  test_accuracy.reset_states()\n",
        "  test_f1.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    train_step(inp, tar)\n",
        "  for inp,tar in valid_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    valid_step(inp,tar)\n",
        "\n",
        "  for inp,tar in test_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    test_step(inp,tar)\n",
        "\n",
        "  print(f'Loss {train_loss.result():.4f} train_f1 {train_f1.result():.4f} Accuracy {train_accuracy.result():.4f}\\\n",
        "   valid_Loss {valid_loss.result():.4f} valid_Accuracy {valid_accuracy.result():.4f} valid_f1 {valid_f1.result():.4f}  test_Loss {test_loss.result():.4f} test_Accuracy {test_accuracy.result():.4f} test_f1 {test_f1.result():.4f}')\n",
        "\n",
        "  train_accuracies.append(train_accuracy.result())\n",
        "  valid_accuracies.append(valid_accuracy.result())\n",
        "  test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "  train_f1s.append(train_f1.result())\n",
        "  valid_f1s.append(valid_f1.result())\n",
        "  test_f1s.append(test_f1.result())\n"
      ],
      "metadata": {
        "id": "ZgyjgFh9puJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8dab1bc-ab81-4dce-f5b1-8bcad26ce792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.7087 train_f1 0.5145 Accuracy 0.5010   valid_Loss 0.6876 valid_Accuracy 0.5096 valid_f1 0.6713  test_Loss 0.6865 test_Accuracy 0.5144 test_f1 0.6702\n",
            "Loss 0.6836 train_f1 0.5991 Accuracy 0.5456   valid_Loss 0.6807 valid_Accuracy 0.5481 valid_f1 0.2341  test_Loss 0.6797 test_Accuracy 0.5481 test_f1 0.2109\n",
            "Loss 0.6479 train_f1 0.4996 Accuracy 0.6210   valid_Loss 0.7069 valid_Accuracy 0.6442 valid_f1 0.5996  test_Loss 0.5770 test_Accuracy 0.7019 test_f1 0.6177\n",
            "Loss 0.6249 train_f1 0.6399 Accuracy 0.6637   valid_Loss 0.7278 valid_Accuracy 0.6635 valid_f1 0.5325  test_Loss 0.6153 test_Accuracy 0.6779 test_f1 0.5635\n",
            "Loss 0.6214 train_f1 0.6606 Accuracy 0.6776   valid_Loss 0.7071 valid_Accuracy 0.6683 valid_f1 0.6072  test_Loss 0.6318 test_Accuracy 0.6827 test_f1 0.5900\n",
            "Loss 0.5837 train_f1 0.6840 Accuracy 0.7004   valid_Loss 0.6499 valid_Accuracy 0.6683 valid_f1 0.6133  test_Loss 0.5413 test_Accuracy 0.7404 test_f1 0.6906\n",
            "Loss 0.5635 train_f1 0.7193 Accuracy 0.7262   valid_Loss 0.5973 valid_Accuracy 0.7019 valid_f1 0.6212  test_Loss 0.5251 test_Accuracy 0.7644 test_f1 0.7202\n",
            "Loss 0.5032 train_f1 0.7350 Accuracy 0.7480   valid_Loss 0.6395 valid_Accuracy 0.7260 valid_f1 0.6830  test_Loss 0.5444 test_Accuracy 0.7692 test_f1 0.7480\n",
            "Loss 0.4213 train_f1 0.7994 Accuracy 0.8095   valid_Loss 1.0338 valid_Accuracy 0.6298 valid_f1 0.4251  test_Loss 1.0746 test_Accuracy 0.6154 test_f1 0.3676\n",
            "Loss 0.3845 train_f1 0.8229 Accuracy 0.8194   valid_Loss 0.5673 valid_Accuracy 0.7837 valid_f1 0.7825  test_Loss 0.6153 test_Accuracy 0.7452 test_f1 0.7105\n",
            "Loss 0.3379 train_f1 0.8507 Accuracy 0.8522   valid_Loss 0.4400 valid_Accuracy 0.7885 valid_f1 0.7805  test_Loss 0.4909 test_Accuracy 0.7740 test_f1 0.7270\n",
            "Loss 0.2264 train_f1 0.9069 Accuracy 0.9137   valid_Loss 0.7176 valid_Accuracy 0.7163 valid_f1 0.7769  test_Loss 0.6210 test_Accuracy 0.7308 test_f1 0.7791\n",
            "Loss 0.1352 train_f1 0.9377 Accuracy 0.9454   valid_Loss 1.4086 valid_Accuracy 0.7115 valid_f1 0.5811  test_Loss 1.7450 test_Accuracy 0.6779 test_f1 0.5256\n",
            "Loss 0.1861 train_f1 0.9229 Accuracy 0.9256   valid_Loss 0.7343 valid_Accuracy 0.7885 valid_f1 0.7771  test_Loss 0.6881 test_Accuracy 0.8077 test_f1 0.8045\n",
            "Loss 0.0984 train_f1 0.9587 Accuracy 0.9603   valid_Loss 0.9117 valid_Accuracy 0.7644 valid_f1 0.7855  test_Loss 0.8405 test_Accuracy 0.7885 test_f1 0.8006\n",
            "Loss 0.0389 train_f1 0.9882 Accuracy 0.9881   valid_Loss 0.8949 valid_Accuracy 0.7788 valid_f1 0.7796  test_Loss 1.0786 test_Accuracy 0.7788 test_f1 0.7360\n",
            "Loss 0.0452 train_f1 0.9837 Accuracy 0.9841   valid_Loss 1.0138 valid_Accuracy 0.7692 valid_f1 0.7021  test_Loss 1.0829 test_Accuracy 0.7452 test_f1 0.7184\n",
            "Loss 0.0535 train_f1 0.9826 Accuracy 0.9812   valid_Loss 1.0589 valid_Accuracy 0.7788 valid_f1 0.7584  test_Loss 1.1087 test_Accuracy 0.7740 test_f1 0.7408\n",
            "Loss 0.0612 train_f1 0.9680 Accuracy 0.9712   valid_Loss 1.5913 valid_Accuracy 0.6971 valid_f1 0.5911  test_Loss 1.6767 test_Accuracy 0.7163 test_f1 0.6079\n",
            "Loss 0.0528 train_f1 0.9795 Accuracy 0.9792   valid_Loss 1.1991 valid_Accuracy 0.7596 valid_f1 0.7638  test_Loss 1.2709 test_Accuracy 0.7788 test_f1 0.7342\n",
            "Loss 0.0052 train_f1 0.9971 Accuracy 0.9970   valid_Loss 1.4277 valid_Accuracy 0.7500 valid_f1 0.7322  test_Loss 1.5782 test_Accuracy 0.7740 test_f1 0.7393\n",
            "Loss 0.0910 train_f1 0.9746 Accuracy 0.9722   valid_Loss 0.7285 valid_Accuracy 0.7500 valid_f1 0.7217  test_Loss 0.8004 test_Accuracy 0.7500 test_f1 0.6966\n",
            "Loss 0.0310 train_f1 0.9982 Accuracy 0.9980   valid_Loss 1.3843 valid_Accuracy 0.7452 valid_f1 0.7158  test_Loss 1.3840 test_Accuracy 0.7837 test_f1 0.7623\n",
            "Loss 0.0032 train_f1 0.9989 Accuracy 0.9990   valid_Loss 1.5049 valid_Accuracy 0.7740 valid_f1 0.7862  test_Loss 1.4733 test_Accuracy 0.7837 test_f1 0.7739\n",
            "Loss 0.0010 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.7089 valid_Accuracy 0.7596 valid_f1 0.7669  test_Loss 1.6414 test_Accuracy 0.8029 test_f1 0.7792\n",
            "Loss 0.0310 train_f1 0.9870 Accuracy 0.9871   valid_Loss 1.4007 valid_Accuracy 0.7933 valid_f1 0.7884  test_Loss 1.3551 test_Accuracy 0.7981 test_f1 0.7605\n",
            "Loss 0.0014 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.6143 valid_Accuracy 0.7500 valid_f1 0.7505  test_Loss 1.5313 test_Accuracy 0.7885 test_f1 0.7836\n",
            "Loss 0.0248 train_f1 0.9915 Accuracy 0.9911   valid_Loss 1.3766 valid_Accuracy 0.7837 valid_f1 0.7454  test_Loss 1.4094 test_Accuracy 0.7933 test_f1 0.8025\n",
            "Loss 0.0051 train_f1 0.9980 Accuracy 0.9980   valid_Loss 1.4842 valid_Accuracy 0.7885 valid_f1 0.8053  test_Loss 1.4580 test_Accuracy 0.7933 test_f1 0.7784\n",
            "Loss 0.0077 train_f1 0.9958 Accuracy 0.9950   valid_Loss 1.5923 valid_Accuracy 0.7885 valid_f1 0.7453  test_Loss 1.5863 test_Accuracy 0.7837 test_f1 0.7543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save result"
      ],
      "metadata": {
        "id": "jVPyJN6nJxTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Train Accuracies': [i.numpy() for i in train_accuracies],\n",
        "    'Valid Accuracies': [i.numpy() for i in valid_accuracies],\n",
        "    'Test Accuracies': [i.numpy() for i in test_accuracies],\n",
        "    'Train F1 Scores': [i.numpy() for i in train_f1s],\n",
        "    'Valid F1 Scores': [i.numpy() for i in valid_f1s],\n",
        "    'Test F1 Scores': [i.numpy() for i in test_f1s]\n",
        "})\n",
        "\n",
        "excel_file_path = 'LSTM_Wine5.xlsx'\n",
        "df.to_excel(excel_file_path, engine='openpyxl')\n",
        "\n",
        "!cp -r \"/content/LSTM_Wine5.xlsx\" \"/content/drive/MyDrive/datasets/master_thesis/record/\""
      ],
      "metadata": {
        "id": "anqwptlD9k4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "highest_value = max(valid_accuracies)\n",
        "\n",
        "\n",
        "index_of_highest = valid_accuracies.index(highest_value)\n",
        "\n",
        "print(test_accuracies[index_of_highest], test_f1s[index_of_highest])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgT-HdgC9Wa",
        "outputId": "de688476-f157-4e40-f869-7385a943468a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.7980769, shape=(), dtype=float32) tf.Tensor(0.76054966, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PChxLeMJIhFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}