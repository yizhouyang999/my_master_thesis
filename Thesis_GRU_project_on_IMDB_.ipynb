{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4fE1W8OydP"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "n24yBUOo6PYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/MyDrive/datasets/master_thesis/IMDB_Dataset.csv.zip\" \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8jNrVv0V1I",
        "outputId": "16e1c254-d167-41da-da0a-cb0dbe2ed0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'IMDB_Dataset.csv.zip'\n",
        "\n",
        "extract_to_path = ''\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)"
      ],
      "metadata": {
        "id": "PSy5CxDfMDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"IMDB Dataset.csv\",encoding='ISO-8859-1')\n",
        "train_text_list=[text[:1500] for text in train_df['review']]\n",
        "train_target_list=[text for text in train_df['sentiment']]\n",
        "train_target_list = [1 if x == 'positive' else 0 for x in train_target_list]\n",
        "train_text_negative=[]\n",
        "train_text_positive=[]\n",
        "train_target_negative=[]\n",
        "train_target_positive=[]\n",
        "for idx, y in enumerate(train_target_list):\n",
        "  if y==0:\n",
        "    train_text_negative.append(train_text_list[idx])\n",
        "    train_target_negative.append(train_target_list[idx])\n",
        "  else:\n",
        "    train_text_positive.append(train_text_list[idx])\n",
        "    train_target_positive.append(train_target_list[idx])\n",
        "\n",
        "\n",
        "import random\n",
        "negative_indices = random.sample(range(len(train_text_negative)), 700)\n",
        "positive_indices = random.sample(range(len(train_text_positive)), 700)\n",
        "\n",
        "train_text_list=[train_text_negative[i] for i in negative_indices[:500]]+[train_text_positive[i] for i in positive_indices[:500]]\n",
        "train_target_list=[train_target_negative[i] for i in negative_indices[:500]]+[train_target_positive[i] for i in positive_indices[:500]]\n",
        "\n",
        "valid_text_list=[train_text_negative[i] for i in negative_indices[500:600]]+[train_text_positive[i] for i in positive_indices[500:600]]\n",
        "valid_target_list=[train_target_negative[i] for i in negative_indices[500:600]]+[train_target_positive[i] for i in positive_indices[500:600]]\n",
        "\n",
        "test_text_list=[train_text_negative[i] for i in negative_indices[600:]]+[train_text_positive[i] for i in positive_indices[600:]]\n",
        "test_target_list=[train_target_negative[i] for i in negative_indices[600:]]+[train_target_positive[i] for i in positive_indices[600:]]\n",
        "\n",
        "text_list=[text for text in train_text_list]+[text for text in valid_text_list]+[text for text in test_text_list]\n",
        "\n",
        "train_text_array=np.array(train_text_list)\n",
        "train_target_array=np.array(train_target_list)\n",
        "\n",
        "valid_text_array=np.array(valid_text_list)\n",
        "valid_target_array=np.array(valid_target_list)\n",
        "\n",
        "test_text_array=np.array(test_text_list)\n",
        "test_target_array=np.array(test_target_list)\n",
        "\n",
        "BUFFER_SIZE = len(train_text_list)\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "7yhFJpeBNsqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mh1RBDZPBpq"
      },
      "outputs": [],
      "source": [
        "def clean_and_format_text(input_text):\n",
        "  lower_text = tf.strings.lower(input_text)\n",
        "  cleaned_text = tf.strings.regex_replace(lower_text, '[^ a-z.?!,¿]', '')\n",
        "  spaced_text = tf.strings.regex_replace(cleaned_text, '[.?!,¿]', r' \\0 ')\n",
        "  final_text=tf.strings.strip(spaced_text)\n",
        "  return final_text\n",
        "\n",
        "\n",
        "maximum_vocab_size = 20000\n",
        "\n",
        "text_vectorizer = preprocessing.TextVectorization(\n",
        "    standardize=clean_and_format_text,\n",
        "    max_tokens=maximum_vocab_size\n",
        ")\n",
        "text_vectorizer.adapt(text_list)\n",
        "def pair_tokenizer(input_text, target_text):\n",
        "    processed_input = text_vectorizer(input_text)\n",
        "    return processed_input, target_text\n",
        "\n",
        "def prepare_dataset_batches(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(pair_tokenizer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46N1ttkVJgZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text_array, train_target_array))\n",
        "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_text_array, valid_target_array))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((test_text_array, test_target_array))\n",
        "\n",
        "train_batches=prepare_dataset_batches(train_dataset)\n",
        "valid_batches=prepare_dataset_batches(valid_dataset)\n",
        "test_batches=prepare_dataset_batches(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the Model"
      ],
      "metadata": {
        "id": "FvpToiY6F1ao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byVijLXYVLl9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHGp54ZVNRZ"
      },
      "outputs": [],
      "source": [
        "class TheSelfAttention(keras.layers.Layer):\n",
        "  def __init__(self,embed_size,heads,**kwargs):\n",
        "    super(TheSelfAttention,self).__init__(**kwargs)\n",
        "\n",
        "    self.embedding_dim = embed_size\n",
        "    self.num_heads = heads\n",
        "    self.depth = embed_size//heads\n",
        "\n",
        "    assert self.depth * self.num_heads == self.embedding_dim\n",
        "\n",
        "    self.dense_value = keras.layers.Dense(self.depth, name=\"value\")\n",
        "    self.dense_key = keras.layers.Dense(self.depth, name=\"key\")\n",
        "    self.dense_query = keras.layers.Dense(self.depth, name=\"query\")\n",
        "    self.final_dense = keras.layers.Dense(self.embedding_dim, name=\"output\")\n",
        "\n",
        "  def call(self,values,keys,queries,mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    def split_heads(x):\n",
        "        return tf.reshape(x, (batch_size, tf.shape(x)[1], self.num_heads, self.depth))\n",
        "\n",
        "    values = self.dense_value(split_heads(values))\n",
        "    keys = self.dense_key(split_heads(keys))\n",
        "    queries = self.dense_query(split_heads(queries))\n",
        "\n",
        "    attention_scores = tf.einsum(\"bnhd,bmhd->bhnm\", queries, keys)\n",
        "\n",
        "    attention_weights=tf.nn.softmax(attention_scores/(self.depth**(1/2)),axis=3)\n",
        "    context_layer=tf.reshape(tf.einsum(\"nhql,nlhd->nqhd\", attention_weights,values),(batch_size,tf.shape(queries)[1],self.num_heads*self.depth))\n",
        "    attended_output = self.final_dense(context_layer)\n",
        "    return attended_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzcCvdzHVPD3"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "  def __init__(self,embedding_dim, num_heads, dropout_rate, forward_expansion, **kwargs):\n",
        "    super(TransformerBlock,self).__init__(**kwargs)\n",
        "    self.self_attention=TheSelfAttention(embedding_dim, num_heads)\n",
        "    self.norm1=keras.layers.LayerNormalization()\n",
        "    self.norm2=keras.layers.LayerNormalization()\n",
        "\n",
        "    self.feed_forward=tf.keras.Sequential([\n",
        "        keras.layers.Dense(forward_expansion * embedding_dim,input_shape=(None,embed_size)),\n",
        "        keras.layers.Activation(\"relu\"),\n",
        "        keras.layers.Dense(embedding_dim)\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    self.dropout=keras.layers.Dropout(dropout)\n",
        "  def call(self,values,keys,queries,mask,training):\n",
        "    attention_output=self.self_attention(values,keys,queries,mask)\n",
        "\n",
        "    out1=self.dropout(self.norm1(attention_output+queries))\n",
        "    forward_output=self.feed_forward(out1)\n",
        "    out2=self.dropout(self.norm2(forward_output+out1),training=training)\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtoKfz3MVRCL"
      },
      "outputs": [],
      "source": [
        "class Model(keras.layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      embedding_dim,\n",
        "      num_layers,\n",
        "      num_heads,\n",
        "      forward_expansion,\n",
        "      dropout_rate,\n",
        "      max_length,\n",
        "      **kwargs\n",
        "    ):\n",
        "    super(Model,self).__init__(**kwargs)\n",
        "    self.embed_size=embed_size\n",
        "    self.token_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.position_embedding = keras.layers.Embedding(max_length, output_dim=embedding_dim)\n",
        "    self.transformer_layers = [TransformerBlock(embedding_dim, num_heads, dropout_rate, forward_expansion) for _ in range(num_layers)]\n",
        "    self.dropout=keras.layers.Dropout(dropout_rate)\n",
        "    self.final_layer=tf.keras.layers.GRU(2,activation='softmax')\n",
        "\n",
        "  def call(self,inputs,mask,training):\n",
        "    batch_size=tf.shape(inputs)[0]\n",
        "    seq_len=tf.shape(inputs)[1]\n",
        "\n",
        "    positions=tf.range(0,seq_len)\n",
        "    positions=tf.reshape(positions,(1,seq_len))\n",
        "    positions=tf.tile(positions,[batch_size,1])\n",
        "\n",
        "    positions = self.position_embedding(positions)\n",
        "    x = self.token_embedding(inputs) + positions\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for layer in self.transformer_layers:\n",
        "      x=layer(x,x,x,mask,training=training)\n",
        "    x = self.dropout(x, training=training)\n",
        "    output = self.final_layer(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and Training"
      ],
      "metadata": {
        "id": "UW2bUdMlF6G3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudIcRIrVS7T"
      },
      "outputs": [],
      "source": [
        "num_epochs=30\n",
        "learning_rate=4e-5\n",
        "src_vocab_size=maximum_vocab_size\n",
        "embed_size=512\n",
        "heads=8\n",
        "num_encoder_layers=4\n",
        "dropout=0.1\n",
        "max_length=100\n",
        "forward_expansion=2\n",
        "\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate)\n",
        "loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_accuracies=[]\n",
        "valid_accuracies=[]\n",
        "test_accuracies=[]\n",
        "\n",
        "train_f1s=[]\n",
        "valid_f1s=[]\n",
        "test_f1s=[]\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.cast(tf.shape(accuracies)[0],dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "train_f1=tf.keras.metrics.Mean(name='train_f1')\n",
        "\n",
        "valid_loss=tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy=tf.keras.metrics.Mean(name='valid_accuracy')\n",
        "valid_f1=tf.keras.metrics.Mean(name='valid_f1')\n",
        "\n",
        "test_loss=tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy=tf.keras.metrics.Mean(name='test_accuracy')\n",
        "test_f1=tf.keras.metrics.Mean(name='test_f1')\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_function(real, pred):\n",
        "\n",
        "    predicted_classes = tf.argmax(pred, axis=1)\n",
        "\n",
        "    TP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 0), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 0)), dtype=tf.float32))\n",
        "\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "\n",
        "    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n",
        "    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n",
        "\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "\n",
        "    f1_score = tf.where(tf.math.is_nan(f1_score), tf.zeros_like(f1_score), f1_score)\n",
        "\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "KiJeDEnhptJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(\n",
        "          src_vocab_size,\n",
        "          embed_size,\n",
        "          num_encoder_layers,\n",
        "          heads,\n",
        "          forward_expansion,\n",
        "          dropout,\n",
        "          max_length\n",
        "      )\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp_data,target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output=model(inp_data,None,True)\n",
        "    loss=loss_object(target,output)\n",
        "  gradients=tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(target,output))\n",
        "  train_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(accuracy_function(target,output))\n",
        "  valid_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def test_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  test_loss(loss)\n",
        "  test_accuracy(accuracy_function(target,output))\n",
        "  test_f1(f1_score_function(target,output))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  train_f1.reset_states()\n",
        "\n",
        "  valid_accuracy.reset_states()\n",
        "  valid_f1.reset_states()\n",
        "  valid_loss.reset_states()\n",
        "\n",
        "  test_accuracy.reset_states()\n",
        "  test_f1.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    train_step(inp, tar)\n",
        "  for inp,tar in valid_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    valid_step(inp,tar)\n",
        "\n",
        "  for inp,tar in test_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    test_step(inp,tar)\n",
        "\n",
        "  print(f'Loss {train_loss.result():.4f} train_f1 {train_f1.result():.4f} Accuracy {train_accuracy.result():.4f}\\\n",
        "   valid_Loss {valid_loss.result():.4f} valid_Accuracy {valid_accuracy.result():.4f} valid_f1 {valid_f1.result():.4f}  test_Loss {test_loss.result():.4f} test_Accuracy {test_accuracy.result():.4f} test_f1 {test_f1.result():.4f}')\n",
        "\n",
        "  train_accuracies.append(train_accuracy.result())\n",
        "  valid_accuracies.append(valid_accuracy.result())\n",
        "  test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "  train_f1s.append(train_f1.result())\n",
        "  valid_f1s.append(valid_f1.result())\n",
        "  test_f1s.append(test_f1.result())\n"
      ],
      "metadata": {
        "id": "ZgyjgFh9puJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6449b7-52db-4600-8562-898d1e9b0641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.7633 train_f1 0.4135 Accuracy 0.5149   valid_Loss 0.7917 valid_Accuracy 0.4952 valid_f1 0.0000  test_Loss 0.7882 test_Accuracy 0.4904 test_f1 0.0000\n",
            "Loss 0.6819 train_f1 0.4486 Accuracy 0.5625   valid_Loss 0.6628 valid_Accuracy 0.6058 valid_f1 0.3972  test_Loss 0.6814 test_Accuracy 0.5288 test_f1 0.2392\n",
            "Loss 0.6896 train_f1 0.5066 Accuracy 0.5774   valid_Loss 0.6791 valid_Accuracy 0.5625 valid_f1 0.4267  test_Loss 0.7230 test_Accuracy 0.5529 test_f1 0.5478\n",
            "Loss 0.6497 train_f1 0.5263 Accuracy 0.6131   valid_Loss 0.6695 valid_Accuracy 0.5865 valid_f1 0.3507  test_Loss 0.6854 test_Accuracy 0.5673 test_f1 0.3353\n",
            "Loss 0.5495 train_f1 0.6414 Accuracy 0.7083   valid_Loss 0.6038 valid_Accuracy 0.6875 valid_f1 0.5501  test_Loss 0.6699 test_Accuracy 0.5962 test_f1 0.3989\n",
            "Loss 0.4180 train_f1 0.7959 Accuracy 0.8085   valid_Loss 0.8301 valid_Accuracy 0.6779 valid_f1 0.5270  test_Loss 0.9763 test_Accuracy 0.6202 test_f1 0.5043\n",
            "Loss 0.2396 train_f1 0.8851 Accuracy 0.8919   valid_Loss 0.6308 valid_Accuracy 0.8077 valid_f1 0.8106  test_Loss 0.8722 test_Accuracy 0.7260 test_f1 0.7108\n",
            "Loss 0.1777 train_f1 0.9175 Accuracy 0.9226   valid_Loss 0.7348 valid_Accuracy 0.8125 valid_f1 0.8224  test_Loss 1.1878 test_Accuracy 0.7452 test_f1 0.7633\n",
            "Loss 0.1067 train_f1 0.9477 Accuracy 0.9554   valid_Loss 0.9412 valid_Accuracy 0.7500 valid_f1 0.6740  test_Loss 1.3352 test_Accuracy 0.7548 test_f1 0.7510\n",
            "Loss 0.1043 train_f1 0.9560 Accuracy 0.9554   valid_Loss 0.7907 valid_Accuracy 0.7644 valid_f1 0.7324  test_Loss 1.2126 test_Accuracy 0.7548 test_f1 0.7590\n",
            "Loss 0.0733 train_f1 0.9731 Accuracy 0.9772   valid_Loss 1.4670 valid_Accuracy 0.6731 valid_f1 0.5494  test_Loss 1.7064 test_Accuracy 0.6779 test_f1 0.5610\n",
            "Loss 0.1587 train_f1 0.9340 Accuracy 0.9385   valid_Loss 0.7584 valid_Accuracy 0.7885 valid_f1 0.8102  test_Loss 1.1560 test_Accuracy 0.7019 test_f1 0.7178\n",
            "Loss 0.0258 train_f1 0.9926 Accuracy 0.9940   valid_Loss 0.8299 valid_Accuracy 0.8269 valid_f1 0.7947  test_Loss 1.5067 test_Accuracy 0.7404 test_f1 0.7386\n",
            "Loss 0.0200 train_f1 0.9910 Accuracy 0.9901   valid_Loss 0.9865 valid_Accuracy 0.8029 valid_f1 0.7819  test_Loss 1.7240 test_Accuracy 0.7260 test_f1 0.7079\n",
            "Loss 0.0074 train_f1 0.9980 Accuracy 0.9980   valid_Loss 1.2062 valid_Accuracy 0.7500 valid_f1 0.7088  test_Loss 1.7351 test_Accuracy 0.7548 test_f1 0.7181\n",
            "Loss 0.0081 train_f1 0.9976 Accuracy 0.9980   valid_Loss 1.0300 valid_Accuracy 0.8125 valid_f1 0.8070  test_Loss 1.9695 test_Accuracy 0.7452 test_f1 0.7272\n",
            "Loss 0.0030 train_f1 0.9992 Accuracy 0.9990   valid_Loss 1.1258 valid_Accuracy 0.7981 valid_f1 0.7969  test_Loss 1.9140 test_Accuracy 0.7404 test_f1 0.7308\n",
            "Loss 0.0143 train_f1 0.9922 Accuracy 0.9940   valid_Loss 1.2717 valid_Accuracy 0.8029 valid_f1 0.7986  test_Loss 2.0986 test_Accuracy 0.7596 test_f1 0.7452\n",
            "Loss 0.0269 train_f1 0.9826 Accuracy 0.9851   valid_Loss 1.2475 valid_Accuracy 0.8077 valid_f1 0.8133  test_Loss 2.0781 test_Accuracy 0.7644 test_f1 0.7823\n",
            "Loss 0.0112 train_f1 0.9960 Accuracy 0.9960   valid_Loss 1.1383 valid_Accuracy 0.7981 valid_f1 0.8016  test_Loss 2.0145 test_Accuracy 0.7548 test_f1 0.7563\n",
            "Loss 0.0003 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.1515 valid_Accuracy 0.8173 valid_f1 0.8273  test_Loss 2.0131 test_Accuracy 0.7596 test_f1 0.7649\n",
            "Loss 0.0005 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.2376 valid_Accuracy 0.8125 valid_f1 0.8003  test_Loss 2.0499 test_Accuracy 0.7404 test_f1 0.7352\n",
            "Loss 0.0002 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.2502 valid_Accuracy 0.8029 valid_f1 0.8130  test_Loss 2.0868 test_Accuracy 0.7548 test_f1 0.7550\n",
            "Loss 0.0002 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.3373 valid_Accuracy 0.8029 valid_f1 0.8075  test_Loss 2.0824 test_Accuracy 0.7452 test_f1 0.7451\n",
            "Loss 0.0026 train_f1 0.9982 Accuracy 0.9980   valid_Loss 1.5415 valid_Accuracy 0.7837 valid_f1 0.8058  test_Loss 2.3552 test_Accuracy 0.7356 test_f1 0.7573\n",
            "Loss 0.0005 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.1851 valid_Accuracy 0.8221 valid_f1 0.8221  test_Loss 2.2851 test_Accuracy 0.7404 test_f1 0.7320\n",
            "Loss 0.0001 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.1848 valid_Accuracy 0.8269 valid_f1 0.8250  test_Loss 2.3575 test_Accuracy 0.7308 test_f1 0.7250\n",
            "Loss 0.0005 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.3792 valid_Accuracy 0.7981 valid_f1 0.7577  test_Loss 2.2976 test_Accuracy 0.7404 test_f1 0.6953\n",
            "Loss 0.0001 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.2783 valid_Accuracy 0.8173 valid_f1 0.8019  test_Loss 2.2552 test_Accuracy 0.7500 test_f1 0.7497\n",
            "Loss 0.0002 train_f1 1.0000 Accuracy 1.0000   valid_Loss 1.2402 valid_Accuracy 0.8125 valid_f1 0.8173  test_Loss 2.3504 test_Accuracy 0.7596 test_f1 0.7646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save result"
      ],
      "metadata": {
        "id": "jVPyJN6nJxTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Train Accuracies': [i.numpy() for i in train_accuracies],\n",
        "    'Valid Accuracies': [i.numpy() for i in valid_accuracies],\n",
        "    'Test Accuracies': [i.numpy() for i in test_accuracies],\n",
        "    'Train F1 Scores': [i.numpy() for i in train_f1s],\n",
        "    'Valid F1 Scores': [i.numpy() for i in valid_f1s],\n",
        "    'Test F1 Scores': [i.numpy() for i in test_f1s]\n",
        "})\n",
        "\n",
        "excel_file_path = 'GRU_IMDB5.xlsx'\n",
        "df.to_excel(excel_file_path, engine='openpyxl')\n",
        "\n",
        "!cp -r \"/content/GRU_IMDB5.xlsx\" \"/content/drive/MyDrive/datasets/master_thesis/record/\""
      ],
      "metadata": {
        "id": "anqwptlD9k4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "highest_value = max(valid_accuracies)\n",
        "\n",
        "\n",
        "index_of_highest = valid_accuracies.index(highest_value)\n",
        "\n",
        "print(test_accuracies[index_of_highest], test_f1s[index_of_highest])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgT-HdgC9Wa",
        "outputId": "2dacf2ee-9bdc-4c53-92a3-cc70ef8b1347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.74038464, shape=(), dtype=float32) tf.Tensor(0.73855484, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}