{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4fE1W8OydP"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "n24yBUOo6PYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/MyDrive/datasets/master_thesis/IMDB_Dataset.csv.zip\" \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8jNrVv0V1I",
        "outputId": "a204ad97-d9a1-4abb-a1a2-33cab34f84aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'IMDB_Dataset.csv.zip'\n",
        "\n",
        "extract_to_path = ''\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)"
      ],
      "metadata": {
        "id": "PSy5CxDfMDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"IMDB Dataset.csv\",encoding='ISO-8859-1')\n",
        "train_text_list=[text[:1500] for text in train_df['review']]\n",
        "train_target_list=[text for text in train_df['sentiment']]\n",
        "train_target_list = [1 if x == 'positive' else 0 for x in train_target_list]\n",
        "train_text_negative=[]\n",
        "train_text_positive=[]\n",
        "train_target_negative=[]\n",
        "train_target_positive=[]\n",
        "for idx, y in enumerate(train_target_list):\n",
        "  if y==0:\n",
        "    train_text_negative.append(train_text_list[idx])\n",
        "    train_target_negative.append(train_target_list[idx])\n",
        "  else:\n",
        "    train_text_positive.append(train_text_list[idx])\n",
        "    train_target_positive.append(train_target_list[idx])\n",
        "\n",
        "\n",
        "import random\n",
        "negative_indices = random.sample(range(len(train_text_negative)), 700)\n",
        "positive_indices = random.sample(range(len(train_text_positive)), 700)\n",
        "\n",
        "train_text_list=[train_text_negative[i] for i in negative_indices[:500]]+[train_text_positive[i] for i in positive_indices[:500]]\n",
        "train_target_list=[train_target_negative[i] for i in negative_indices[:500]]+[train_target_positive[i] for i in positive_indices[:500]]\n",
        "\n",
        "valid_text_list=[train_text_negative[i] for i in negative_indices[500:600]]+[train_text_positive[i] for i in positive_indices[500:600]]\n",
        "valid_target_list=[train_target_negative[i] for i in negative_indices[500:600]]+[train_target_positive[i] for i in positive_indices[500:600]]\n",
        "\n",
        "test_text_list=[train_text_negative[i] for i in negative_indices[600:]]+[train_text_positive[i] for i in positive_indices[600:]]\n",
        "test_target_list=[train_target_negative[i] for i in negative_indices[600:]]+[train_target_positive[i] for i in positive_indices[600:]]\n",
        "\n",
        "text_list=[text for text in train_text_list]+[text for text in valid_text_list]+[text for text in test_text_list]\n",
        "\n",
        "train_text_array=np.array(train_text_list)\n",
        "train_target_array=np.array(train_target_list)\n",
        "\n",
        "valid_text_array=np.array(valid_text_list)\n",
        "valid_target_array=np.array(valid_target_list)\n",
        "\n",
        "test_text_array=np.array(test_text_list)\n",
        "test_target_array=np.array(test_target_list)\n",
        "\n",
        "BUFFER_SIZE = len(train_text_list)\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "7yhFJpeBNsqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mh1RBDZPBpq"
      },
      "outputs": [],
      "source": [
        "def clean_and_format_text(input_text):\n",
        "  lower_text = tf.strings.lower(input_text)\n",
        "  cleaned_text = tf.strings.regex_replace(lower_text, '[^ a-z.?!,¿]', '')\n",
        "  spaced_text = tf.strings.regex_replace(cleaned_text, '[.?!,¿]', r' \\0 ')\n",
        "  final_text=tf.strings.strip(spaced_text)\n",
        "  return final_text\n",
        "\n",
        "\n",
        "maximum_vocab_size = 20000\n",
        "\n",
        "text_vectorizer = preprocessing.TextVectorization(\n",
        "    standardize=clean_and_format_text,\n",
        "    max_tokens=maximum_vocab_size\n",
        ")\n",
        "text_vectorizer.adapt(text_list)\n",
        "def pair_tokenizer(input_text, target_text):\n",
        "    processed_input = text_vectorizer(input_text)\n",
        "    return processed_input, target_text\n",
        "\n",
        "def prepare_dataset_batches(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(pair_tokenizer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46N1ttkVJgZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text_array, train_target_array))\n",
        "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_text_array, valid_target_array))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((test_text_array, test_target_array))\n",
        "\n",
        "train_batches=prepare_dataset_batches(train_dataset)\n",
        "valid_batches=prepare_dataset_batches(valid_dataset)\n",
        "test_batches=prepare_dataset_batches(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the Model"
      ],
      "metadata": {
        "id": "yFUh7Xb3HAd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byVijLXYVLl9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHGp54ZVNRZ"
      },
      "outputs": [],
      "source": [
        "class TheSelfAttention(keras.layers.Layer):\n",
        "  def __init__(self,embed_size,heads,**kwargs):\n",
        "    super(TheSelfAttention,self).__init__(**kwargs)\n",
        "\n",
        "    self.embedding_dim = embed_size\n",
        "    self.num_heads = heads\n",
        "    self.depth = embed_size//heads\n",
        "\n",
        "    assert self.depth * self.num_heads == self.embedding_dim\n",
        "\n",
        "    self.dense_value = keras.layers.Dense(self.depth, name=\"value\")\n",
        "    self.dense_key = keras.layers.Dense(self.depth, name=\"key\")\n",
        "    self.dense_query = keras.layers.Dense(self.depth, name=\"query\")\n",
        "    self.final_dense = keras.layers.Dense(self.embedding_dim, name=\"output\")\n",
        "\n",
        "  def call(self,values,keys,queries,mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    def split_heads(x):\n",
        "        return tf.reshape(x, (batch_size, tf.shape(x)[1], self.num_heads, self.depth))\n",
        "\n",
        "    values = self.dense_value(split_heads(values))\n",
        "    keys = self.dense_key(split_heads(keys))\n",
        "    queries = self.dense_query(split_heads(queries))\n",
        "\n",
        "    attention_scores = tf.einsum(\"bnhd,bmhd->bhnm\", queries, keys)\n",
        "\n",
        "    attention_weights=tf.nn.softmax(attention_scores/(self.depth**(1/2)),axis=3)\n",
        "    context_layer=tf.reshape(tf.einsum(\"nhql,nlhd->nqhd\", attention_weights,values),(batch_size,tf.shape(queries)[1],self.num_heads*self.depth))\n",
        "    attended_output = self.final_dense(context_layer)\n",
        "    return attended_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzcCvdzHVPD3"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "  def __init__(self,embedding_dim, num_heads, dropout_rate, forward_expansion, **kwargs):\n",
        "    super(TransformerBlock,self).__init__(**kwargs)\n",
        "    self.self_attention=TheSelfAttention(embedding_dim, num_heads)\n",
        "    self.norm1=keras.layers.LayerNormalization()\n",
        "    self.norm2=keras.layers.LayerNormalization()\n",
        "\n",
        "    self.feed_forward=tf.keras.Sequential([\n",
        "        keras.layers.Dense(forward_expansion * embedding_dim,input_shape=(None,embed_size)),\n",
        "        keras.layers.Activation(\"relu\"),\n",
        "        keras.layers.Dense(embedding_dim)\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    self.dropout=keras.layers.Dropout(dropout)\n",
        "  def call(self,values,keys,queries,mask,training):\n",
        "    attention_output=self.self_attention(values,keys,queries,mask)\n",
        "\n",
        "    out1=self.dropout(self.norm1(attention_output+queries))\n",
        "    forward_output=self.feed_forward(out1)\n",
        "    out2=self.dropout(self.norm2(forward_output+out1),training=training)\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtoKfz3MVRCL"
      },
      "outputs": [],
      "source": [
        "class Model(keras.layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      embedding_dim,\n",
        "      num_layers,\n",
        "      num_heads,\n",
        "      forward_expansion,\n",
        "      dropout_rate,\n",
        "      max_length,\n",
        "      **kwargs\n",
        "    ):\n",
        "    super(Model,self).__init__(**kwargs)\n",
        "    self.embed_size=embed_size\n",
        "    self.token_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.position_embedding = keras.layers.Embedding(max_length, output_dim=embedding_dim)\n",
        "    self.transformer_layers = [TransformerBlock(embedding_dim, num_heads, dropout_rate, forward_expansion) for _ in range(num_layers)]\n",
        "    self.dropout=keras.layers.Dropout(dropout_rate)\n",
        "    self.final_layer=tf.keras.layers.LSTM(2,activation='softmax')\n",
        "\n",
        "  def call(self,inputs,mask,training):\n",
        "    batch_size=tf.shape(inputs)[0]\n",
        "    seq_len=tf.shape(inputs)[1]\n",
        "\n",
        "    positions=tf.range(0,seq_len)\n",
        "    positions=tf.reshape(positions,(1,seq_len))\n",
        "    positions=tf.tile(positions,[batch_size,1])\n",
        "\n",
        "    positions = self.position_embedding(positions)\n",
        "    x = self.token_embedding(inputs) + positions\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for layer in self.transformer_layers:\n",
        "      x=layer(x,x,x,mask,training=training)\n",
        "    x = self.dropout(x, training=training)\n",
        "    output = self.final_layer(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and Training"
      ],
      "metadata": {
        "id": "bFZWMfXvHD5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudIcRIrVS7T"
      },
      "outputs": [],
      "source": [
        "num_epochs=30\n",
        "learning_rate=4e-5\n",
        "src_vocab_size=maximum_vocab_size\n",
        "embed_size=512\n",
        "heads=8\n",
        "num_encoder_layers=4\n",
        "dropout=0.1\n",
        "max_length=100\n",
        "forward_expansion=2\n",
        "\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate)\n",
        "loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_accuracies=[]\n",
        "valid_accuracies=[]\n",
        "test_accuracies=[]\n",
        "\n",
        "train_f1s=[]\n",
        "valid_f1s=[]\n",
        "test_f1s=[]\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.cast(tf.shape(accuracies)[0],dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "train_f1=tf.keras.metrics.Mean(name='train_f1')\n",
        "\n",
        "valid_loss=tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy=tf.keras.metrics.Mean(name='valid_accuracy')\n",
        "valid_f1=tf.keras.metrics.Mean(name='valid_f1')\n",
        "\n",
        "test_loss=tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy=tf.keras.metrics.Mean(name='test_accuracy')\n",
        "test_f1=tf.keras.metrics.Mean(name='test_f1')\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_function(real, pred):\n",
        "\n",
        "    predicted_classes = tf.argmax(pred, axis=1)\n",
        "\n",
        "    TP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 0), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 0)), dtype=tf.float32))\n",
        "\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "\n",
        "    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n",
        "    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n",
        "\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "\n",
        "    f1_score = tf.where(tf.math.is_nan(f1_score), tf.zeros_like(f1_score), f1_score)\n",
        "\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "KiJeDEnhptJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(\n",
        "          src_vocab_size,\n",
        "          embed_size,\n",
        "          num_encoder_layers,\n",
        "          heads,\n",
        "          forward_expansion,\n",
        "          dropout,\n",
        "          max_length\n",
        "      )\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp_data,target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output=model(inp_data,None,True)\n",
        "    loss=loss_object(target,output)\n",
        "  gradients=tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(target,output))\n",
        "  train_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(accuracy_function(target,output))\n",
        "  valid_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def test_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  test_loss(loss)\n",
        "  test_accuracy(accuracy_function(target,output))\n",
        "  test_f1(f1_score_function(target,output))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  train_f1.reset_states()\n",
        "\n",
        "  valid_accuracy.reset_states()\n",
        "  valid_f1.reset_states()\n",
        "  valid_loss.reset_states()\n",
        "\n",
        "  test_accuracy.reset_states()\n",
        "  test_f1.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    train_step(inp, tar)\n",
        "  for inp,tar in valid_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    valid_step(inp,tar)\n",
        "\n",
        "  for inp,tar in test_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    test_step(inp,tar)\n",
        "\n",
        "  print(f'Loss {train_loss.result():.4f} train_f1 {train_f1.result():.4f} Accuracy {train_accuracy.result():.4f}\\\n",
        "   valid_Loss {valid_loss.result():.4f} valid_Accuracy {valid_accuracy.result():.4f} valid_f1 {valid_f1.result():.4f}  test_Loss {test_loss.result():.4f} test_Accuracy {test_accuracy.result():.4f} test_f1 {test_f1.result():.4f}')\n",
        "\n",
        "  train_accuracies.append(train_accuracy.result())\n",
        "  valid_accuracies.append(valid_accuracy.result())\n",
        "  test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "  train_f1s.append(train_f1.result())\n",
        "  valid_f1s.append(valid_f1.result())\n",
        "  test_f1s.append(test_f1.result())\n"
      ],
      "metadata": {
        "id": "ZgyjgFh9puJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30768e9e-492e-414a-ef60-b87dfb801f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.7023 train_f1 0.4994 Accuracy 0.5050   valid_Loss 0.6932 valid_Accuracy 0.4904 valid_f1 0.6430  test_Loss 0.6931 test_Accuracy 0.5048 test_f1 0.6612\n",
            "Loss 0.6929 train_f1 0.6259 Accuracy 0.5139   valid_Loss 0.6931 valid_Accuracy 0.5048 valid_f1 0.6592  test_Loss 0.6931 test_Accuracy 0.4952 test_f1 0.6458\n",
            "Loss 0.6931 train_f1 0.6081 Accuracy 0.5248   valid_Loss 0.6931 valid_Accuracy 0.5144 valid_f1 0.6445  test_Loss 0.6931 test_Accuracy 0.5192 test_f1 0.6409\n",
            "Loss 0.6933 train_f1 0.6139 Accuracy 0.4970   valid_Loss 0.6932 valid_Accuracy 0.4808 valid_f1 0.6357  test_Loss 0.6929 test_Accuracy 0.5096 test_f1 0.6633\n",
            "Loss 0.6934 train_f1 0.5708 Accuracy 0.5040   valid_Loss 0.6932 valid_Accuracy 0.5000 valid_f1 0.5393  test_Loss 0.6931 test_Accuracy 0.5048 test_f1 0.5753\n",
            "Loss 0.6928 train_f1 0.4940 Accuracy 0.5397   valid_Loss 0.6926 valid_Accuracy 0.4904 valid_f1 0.0118  test_Loss 0.6932 test_Accuracy 0.4952 test_f1 0.0154\n",
            "Loss 0.6932 train_f1 0.4874 Accuracy 0.4881   valid_Loss 0.6923 valid_Accuracy 0.5000 valid_f1 0.6610  test_Loss 0.6923 test_Accuracy 0.4952 test_f1 0.6570\n",
            "Loss 0.6923 train_f1 0.5133 Accuracy 0.5278   valid_Loss 0.6930 valid_Accuracy 0.4952 valid_f1 0.0000  test_Loss 0.6931 test_Accuracy 0.4856 test_f1 0.0000\n",
            "Loss 0.6925 train_f1 0.2792 Accuracy 0.5149   valid_Loss 0.6932 valid_Accuracy 0.5865 valid_f1 0.3739  test_Loss 0.6906 test_Accuracy 0.5721 test_f1 0.3778\n",
            "Loss 0.6905 train_f1 0.4428 Accuracy 0.5526   valid_Loss 0.6830 valid_Accuracy 0.5337 valid_f1 0.4310  test_Loss 0.6730 test_Accuracy 0.5769 test_f1 0.4847\n",
            "Loss 0.6711 train_f1 0.6441 Accuracy 0.5397   valid_Loss 0.6601 valid_Accuracy 0.5625 valid_f1 0.6725  test_Loss 0.6622 test_Accuracy 0.5769 test_f1 0.6877\n",
            "Loss 0.6248 train_f1 0.6771 Accuracy 0.6319   valid_Loss 0.7438 valid_Accuracy 0.5913 valid_f1 0.3869  test_Loss 0.7156 test_Accuracy 0.6106 test_f1 0.3906\n",
            "Loss 0.6374 train_f1 0.3307 Accuracy 0.6270   valid_Loss 0.6922 valid_Accuracy 0.5048 valid_f1 0.0000  test_Loss 0.6919 test_Accuracy 0.4952 test_f1 0.0000\n",
            "Loss 0.6886 train_f1 0.0000 Accuracy 0.5000   valid_Loss 0.7110 valid_Accuracy 0.4904 valid_f1 0.0000  test_Loss 0.6938 test_Accuracy 0.5048 test_f1 0.0000\n",
            "Loss 0.6326 train_f1 0.0000 Accuracy 0.5020   valid_Loss 4.9534 valid_Accuracy 0.5000 valid_f1 0.0000  test_Loss 4.9167 test_Accuracy 0.4904 test_f1 0.0000\n",
            "Loss 0.7402 train_f1 0.6111 Accuracy 0.5218   valid_Loss 0.6931 valid_Accuracy 0.4952 valid_f1 0.6564  test_Loss 0.6931 test_Accuracy 0.5048 test_f1 0.6572\n",
            "Loss 0.6929 train_f1 0.6520 Accuracy 0.4990   valid_Loss 0.6931 valid_Accuracy 0.4952 valid_f1 0.6530  test_Loss 0.6930 test_Accuracy 0.5048 test_f1 0.6609\n",
            "Loss 0.6927 train_f1 0.6562 Accuracy 0.5020   valid_Loss 0.6930 valid_Accuracy 0.5048 valid_f1 0.6672  test_Loss 0.6930 test_Accuracy 0.5000 test_f1 0.6543\n",
            "Loss 0.6919 train_f1 0.6548 Accuracy 0.4990   valid_Loss 0.6924 valid_Accuracy 0.5144 valid_f1 0.6666  test_Loss 0.6924 test_Accuracy 0.5096 test_f1 0.6683\n",
            "Loss 0.6831 train_f1 0.6534 Accuracy 0.4990   valid_Loss 0.7154 valid_Accuracy 0.5000 valid_f1 0.6575  test_Loss 0.7113 test_Accuracy 0.4952 test_f1 0.6565\n",
            "Loss 0.5989 train_f1 0.6591 Accuracy 0.5010   valid_Loss 0.7570 valid_Accuracy 0.5096 valid_f1 0.6636  test_Loss 0.7682 test_Accuracy 0.5144 test_f1 0.6653\n",
            "Loss 0.5072 train_f1 0.7111 Accuracy 0.6081   valid_Loss 0.7631 valid_Accuracy 0.6442 valid_f1 0.6052  test_Loss 0.7756 test_Accuracy 0.6875 test_f1 0.6981\n",
            "Loss 0.5312 train_f1 0.7974 Accuracy 0.7748   valid_Loss 1.0982 valid_Accuracy 0.6875 valid_f1 0.7047  test_Loss 1.1081 test_Accuracy 0.6779 test_f1 0.7388\n",
            "Loss 0.4420 train_f1 0.7902 Accuracy 0.7421   valid_Loss 1.0700 valid_Accuracy 0.7067 valid_f1 0.7557  test_Loss 1.0729 test_Accuracy 0.6635 test_f1 0.7220\n",
            "Loss 0.3914 train_f1 0.8429 Accuracy 0.8204   valid_Loss 1.3314 valid_Accuracy 0.6587 valid_f1 0.7334  test_Loss 1.5379 test_Accuracy 0.6106 test_f1 0.7013\n",
            "Loss 0.2693 train_f1 0.9384 Accuracy 0.9395   valid_Loss 0.8762 valid_Accuracy 0.6971 valid_f1 0.5978  test_Loss 0.9187 test_Accuracy 0.7308 test_f1 0.6663\n",
            "Loss 0.1545 train_f1 0.9391 Accuracy 0.9395   valid_Loss 0.8848 valid_Accuracy 0.7356 valid_f1 0.7553  test_Loss 0.9939 test_Accuracy 0.7356 test_f1 0.7615\n",
            "Loss 0.0822 train_f1 0.9556 Accuracy 0.9663   valid_Loss 1.0327 valid_Accuracy 0.7933 valid_f1 0.7830  test_Loss 1.3249 test_Accuracy 0.7500 test_f1 0.7564\n",
            "Loss 0.0333 train_f1 0.9860 Accuracy 0.9881   valid_Loss 1.9798 valid_Accuracy 0.7356 valid_f1 0.7711  test_Loss 2.1801 test_Accuracy 0.7163 test_f1 0.7551\n",
            "Loss 0.0172 train_f1 0.9951 Accuracy 0.9950   valid_Loss 1.2408 valid_Accuracy 0.8125 valid_f1 0.8039  test_Loss 1.5446 test_Accuracy 0.7548 test_f1 0.7570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save result"
      ],
      "metadata": {
        "id": "jVPyJN6nJxTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Train Accuracies': [i.numpy() for i in train_accuracies],\n",
        "    'Valid Accuracies': [i.numpy() for i in valid_accuracies],\n",
        "    'Test Accuracies': [i.numpy() for i in test_accuracies],\n",
        "    'Train F1 Scores': [i.numpy() for i in train_f1s],\n",
        "    'Valid F1 Scores': [i.numpy() for i in valid_f1s],\n",
        "    'Test F1 Scores': [i.numpy() for i in test_f1s]\n",
        "})\n",
        "\n",
        "excel_file_path = 'LSTM_IMDB5.xlsx'\n",
        "df.to_excel(excel_file_path, engine='openpyxl')\n",
        "\n",
        "!cp -r \"/content/LSTM_IMDB5.xlsx\" \"/content/drive/MyDrive/datasets/master_thesis/record/\""
      ],
      "metadata": {
        "id": "anqwptlD9k4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "highest_value = max(valid_accuracies)\n",
        "\n",
        "\n",
        "index_of_highest = valid_accuracies.index(highest_value)\n",
        "\n",
        "print(test_accuracies[index_of_highest], test_f1s[index_of_highest])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgT-HdgC9Wa",
        "outputId": "754d74b9-d194-479f-cd0b-335157495441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.7548077, shape=(), dtype=float32) tf.Tensor(0.7569528, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PChxLeMJIhFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}