{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS4fE1W8OydP"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "n24yBUOo6PYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/MyDrive/datasets/master_thesis/DisneylandReviews.csv.zip\" \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8jNrVv0V1I",
        "outputId": "864cacdb-e421-4f17-a8d4-b42d25633c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'DisneylandReviews.csv.zip'\n",
        "\n",
        "extract_to_path = ''\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)"
      ],
      "metadata": {
        "id": "PSy5CxDfMDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"DisneylandReviews.csv\",encoding='ISO-8859-1')\n",
        "train_text_list=[text[:1500] for text in train_df['Review_Text']]\n",
        "train_target_list=[text for text in train_df['Rating']]\n",
        "train_target_list = [1 if x > 3 else 0 for x in train_target_list]\n",
        "train_text_negative=[]\n",
        "train_text_positive=[]\n",
        "train_target_negative=[]\n",
        "train_target_positive=[]\n",
        "for idx, y in enumerate(train_target_list):\n",
        "  if y==0:\n",
        "    train_text_negative.append(train_text_list[idx])\n",
        "    train_target_negative.append(train_target_list[idx])\n",
        "  else:\n",
        "    train_text_positive.append(train_text_list[idx])\n",
        "    train_target_positive.append(train_target_list[idx])\n",
        "\n",
        "\n",
        "import random\n",
        "negative_indices = random.sample(range(len(train_text_negative)), 700)\n",
        "positive_indices = random.sample(range(len(train_text_positive)), 700)\n",
        "\n",
        "train_text_list=[train_text_negative[i] for i in negative_indices[:500]]+[train_text_positive[i] for i in positive_indices[:500]]\n",
        "train_target_list=[train_target_negative[i] for i in negative_indices[:500]]+[train_target_positive[i] for i in positive_indices[:500]]\n",
        "\n",
        "valid_text_list=[train_text_negative[i] for i in negative_indices[500:600]]+[train_text_positive[i] for i in positive_indices[500:600]]\n",
        "valid_target_list=[train_target_negative[i] for i in negative_indices[500:600]]+[train_target_positive[i] for i in positive_indices[500:600]]\n",
        "\n",
        "test_text_list=[train_text_negative[i] for i in negative_indices[600:]]+[train_text_positive[i] for i in positive_indices[600:]]\n",
        "test_target_list=[train_target_negative[i] for i in negative_indices[600:]]+[train_target_positive[i] for i in positive_indices[600:]]\n",
        "\n",
        "text_list=[text for text in train_text_list]+[text for text in valid_text_list]+[text for text in test_text_list]\n",
        "\n",
        "train_text_array=np.array(train_text_list)\n",
        "train_target_array=np.array(train_target_list)\n",
        "\n",
        "valid_text_array=np.array(valid_text_list)\n",
        "valid_target_array=np.array(valid_target_list)\n",
        "\n",
        "test_text_array=np.array(test_text_list)\n",
        "test_target_array=np.array(test_target_list)\n",
        "\n",
        "BUFFER_SIZE = len(train_text_list)\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "7yhFJpeBNsqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mh1RBDZPBpq"
      },
      "outputs": [],
      "source": [
        "def clean_and_format_text(input_text):\n",
        "  lower_text = tf.strings.lower(input_text)\n",
        "  cleaned_text = tf.strings.regex_replace(lower_text, '[^ a-z.?!,¿]', '')\n",
        "  spaced_text = tf.strings.regex_replace(cleaned_text, '[.?!,¿]', r' \\0 ')\n",
        "  final_text=tf.strings.strip(spaced_text)\n",
        "  return final_text\n",
        "\n",
        "\n",
        "maximum_vocab_size = 20000\n",
        "\n",
        "text_vectorizer = preprocessing.TextVectorization(\n",
        "    standardize=clean_and_format_text,\n",
        "    max_tokens=maximum_vocab_size\n",
        ")\n",
        "text_vectorizer.adapt(text_list)\n",
        "def pair_tokenizer(input_text, target_text):\n",
        "    processed_input = text_vectorizer(input_text)\n",
        "    return processed_input, target_text\n",
        "\n",
        "def prepare_dataset_batches(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(pair_tokenizer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46N1ttkVJgZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text_array, train_target_array))\n",
        "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_text_array, valid_target_array))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((test_text_array, test_target_array))\n",
        "\n",
        "train_batches=prepare_dataset_batches(train_dataset)\n",
        "valid_batches=prepare_dataset_batches(valid_dataset)\n",
        "test_batches=prepare_dataset_batches(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the Model"
      ],
      "metadata": {
        "id": "6WcWJ2PNGiiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byVijLXYVLl9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHGp54ZVNRZ"
      },
      "outputs": [],
      "source": [
        "class TheSelfAttention(keras.layers.Layer):\n",
        "  def __init__(self,embed_size,heads,**kwargs):\n",
        "    super(TheSelfAttention,self).__init__(**kwargs)\n",
        "\n",
        "    self.embedding_dim = embed_size\n",
        "    self.num_heads = heads\n",
        "    self.depth = embed_size//heads\n",
        "\n",
        "    assert self.depth * self.num_heads == self.embedding_dim\n",
        "\n",
        "    self.dense_value = keras.layers.Dense(self.depth, name=\"value\")\n",
        "    self.dense_key = keras.layers.Dense(self.depth, name=\"key\")\n",
        "    self.dense_query = keras.layers.Dense(self.depth, name=\"query\")\n",
        "    self.final_dense = keras.layers.Dense(self.embedding_dim, name=\"output\")\n",
        "\n",
        "  def call(self,values,keys,queries,mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    def split_heads(x):\n",
        "        return tf.reshape(x, (batch_size, tf.shape(x)[1], self.num_heads, self.depth))\n",
        "\n",
        "    values = self.dense_value(split_heads(values))\n",
        "    keys = self.dense_key(split_heads(keys))\n",
        "    queries = self.dense_query(split_heads(queries))\n",
        "\n",
        "    attention_scores = tf.einsum(\"bnhd,bmhd->bhnm\", queries, keys)\n",
        "\n",
        "    attention_weights=tf.nn.softmax(attention_scores/(self.depth**(1/2)),axis=3)\n",
        "    context_layer=tf.reshape(tf.einsum(\"nhql,nlhd->nqhd\", attention_weights,values),(batch_size,tf.shape(queries)[1],self.num_heads*self.depth))\n",
        "    attended_output = self.final_dense(context_layer)\n",
        "    return attended_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzcCvdzHVPD3"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "  def __init__(self,embedding_dim, num_heads, dropout_rate, forward_expansion, **kwargs):\n",
        "    super(TransformerBlock,self).__init__(**kwargs)\n",
        "    self.self_attention=TheSelfAttention(embedding_dim, num_heads)\n",
        "    self.norm1=keras.layers.LayerNormalization()\n",
        "    self.norm2=keras.layers.LayerNormalization()\n",
        "\n",
        "    self.feed_forward=tf.keras.Sequential([\n",
        "        keras.layers.Dense(forward_expansion * embedding_dim,input_shape=(None,embed_size)),\n",
        "        keras.layers.Activation(\"relu\"),\n",
        "        keras.layers.Dense(embedding_dim)\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    self.dropout=keras.layers.Dropout(dropout)\n",
        "  def call(self,values,keys,queries,mask,training):\n",
        "    attention_output=self.self_attention(values,keys,queries,mask)\n",
        "\n",
        "    out1=self.dropout(self.norm1(attention_output+queries))\n",
        "    forward_output=self.feed_forward(out1)\n",
        "    out2=self.dropout(self.norm2(forward_output+out1),training=training)\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtoKfz3MVRCL"
      },
      "outputs": [],
      "source": [
        "class Model(keras.layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      embedding_dim,\n",
        "      num_layers,\n",
        "      num_heads,\n",
        "      forward_expansion,\n",
        "      dropout_rate,\n",
        "      max_length,\n",
        "      **kwargs\n",
        "    ):\n",
        "    super(Model,self).__init__(**kwargs)\n",
        "    self.embed_size=embed_size\n",
        "    self.token_embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.position_embedding = keras.layers.Embedding(max_length, output_dim=embedding_dim)\n",
        "    self.transformer_layers = [TransformerBlock(embedding_dim, num_heads, dropout_rate, forward_expansion) for _ in range(num_layers)]\n",
        "    self.dropout=keras.layers.Dropout(dropout_rate)\n",
        "    self.final_layer=tf.keras.layers.GRU(2,activation='softmax')\n",
        "\n",
        "  def call(self,inputs,mask,training):\n",
        "    batch_size=tf.shape(inputs)[0]\n",
        "    seq_len=tf.shape(inputs)[1]\n",
        "\n",
        "    positions=tf.range(0,seq_len)\n",
        "    positions=tf.reshape(positions,(1,seq_len))\n",
        "    positions=tf.tile(positions,[batch_size,1])\n",
        "\n",
        "    positions = self.position_embedding(positions)\n",
        "    x = self.token_embedding(inputs) + positions\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for layer in self.transformer_layers:\n",
        "      x=layer(x,x,x,mask,training=training)\n",
        "    x = self.dropout(x, training=training)\n",
        "    output = self.final_layer(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and Training"
      ],
      "metadata": {
        "id": "NgghkOCoGmHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudIcRIrVS7T"
      },
      "outputs": [],
      "source": [
        "num_epochs=30\n",
        "learning_rate=4e-5\n",
        "src_vocab_size=maximum_vocab_size\n",
        "embed_size=512\n",
        "heads=8\n",
        "num_encoder_layers=4\n",
        "dropout=0.1\n",
        "max_length=100\n",
        "forward_expansion=2\n",
        "\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate)\n",
        "loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_accuracies=[]\n",
        "valid_accuracies=[]\n",
        "test_accuracies=[]\n",
        "\n",
        "train_f1s=[]\n",
        "valid_f1s=[]\n",
        "test_f1s=[]\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.cast(tf.shape(accuracies)[0],dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "train_f1=tf.keras.metrics.Mean(name='train_f1')\n",
        "\n",
        "valid_loss=tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy=tf.keras.metrics.Mean(name='valid_accuracy')\n",
        "valid_f1=tf.keras.metrics.Mean(name='valid_f1')\n",
        "\n",
        "test_loss=tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy=tf.keras.metrics.Mean(name='test_accuracy')\n",
        "test_f1=tf.keras.metrics.Mean(name='test_f1')\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_function(real, pred):\n",
        "\n",
        "    predicted_classes = tf.argmax(pred, axis=1)\n",
        "\n",
        "    TP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 0), tf.equal(predicted_classes, 1)), dtype=tf.float32))\n",
        "    FN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(real, 1), tf.equal(predicted_classes, 0)), dtype=tf.float32))\n",
        "\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "\n",
        "    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n",
        "    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n",
        "\n",
        "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "\n",
        "    f1_score = tf.where(tf.math.is_nan(f1_score), tf.zeros_like(f1_score), f1_score)\n",
        "\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "KiJeDEnhptJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(\n",
        "          src_vocab_size,\n",
        "          embed_size,\n",
        "          num_encoder_layers,\n",
        "          heads,\n",
        "          forward_expansion,\n",
        "          dropout,\n",
        "          max_length\n",
        "      )\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp_data,target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    output=model(inp_data,None,True)\n",
        "    loss=loss_object(target,output)\n",
        "  gradients=tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(target,output))\n",
        "  train_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(accuracy_function(target,output))\n",
        "  valid_f1(f1_score_function(target,output))\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def test_step(inp_data,target):\n",
        "  output=model(inp_data,None,False)\n",
        "  loss=loss_object(target,output)\n",
        "  test_loss(loss)\n",
        "  test_accuracy(accuracy_function(target,output))\n",
        "  test_f1(f1_score_function(target,output))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  train_f1.reset_states()\n",
        "\n",
        "  valid_accuracy.reset_states()\n",
        "  valid_f1.reset_states()\n",
        "  valid_loss.reset_states()\n",
        "\n",
        "  test_accuracy.reset_states()\n",
        "  test_f1.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    train_step(inp, tar)\n",
        "  for inp,tar in valid_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    valid_step(inp,tar)\n",
        "\n",
        "  for inp,tar in test_batches:\n",
        "    tar=tf.cast(tar,dtype=tf.int64)\n",
        "    test_step(inp,tar)\n",
        "\n",
        "  print(f'Loss {train_loss.result():.4f} train_f1 {train_f1.result():.4f} Accuracy {train_accuracy.result():.4f}\\\n",
        "   valid_Loss {valid_loss.result():.4f} valid_Accuracy {valid_accuracy.result():.4f} valid_f1 {valid_f1.result():.4f}  test_Loss {test_loss.result():.4f} test_Accuracy {test_accuracy.result():.4f} test_f1 {test_f1.result():.4f}')\n",
        "\n",
        "  train_accuracies.append(train_accuracy.result())\n",
        "  valid_accuracies.append(valid_accuracy.result())\n",
        "  test_accuracies.append(test_accuracy.result())\n",
        "\n",
        "  train_f1s.append(train_f1.result())\n",
        "  valid_f1s.append(valid_f1.result())\n",
        "  test_f1s.append(test_f1.result())\n"
      ],
      "metadata": {
        "id": "ZgyjgFh9puJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0fb2a0-ec45-4c64-ea4a-f713e0768584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.7491 train_f1 0.4700 Accuracy 0.5268   valid_Loss 0.7215 valid_Accuracy 0.4856 valid_f1 0.0000  test_Loss 0.6901 test_Accuracy 0.5144 test_f1 0.0374\n",
            "Loss 0.7057 train_f1 0.5089 Accuracy 0.5615   valid_Loss 0.7737 valid_Accuracy 0.5288 valid_f1 0.6343  test_Loss 0.7136 test_Accuracy 0.6010 test_f1 0.7008\n",
            "Loss 0.7208 train_f1 0.4589 Accuracy 0.5575   valid_Loss 0.7483 valid_Accuracy 0.5529 valid_f1 0.6026  test_Loss 0.6767 test_Accuracy 0.6106 test_f1 0.6674\n",
            "Loss 0.6691 train_f1 0.4954 Accuracy 0.5774   valid_Loss 0.6960 valid_Accuracy 0.5769 valid_f1 0.6183  test_Loss 0.6210 test_Accuracy 0.6635 test_f1 0.6877\n",
            "Loss 0.6619 train_f1 0.5239 Accuracy 0.5804   valid_Loss 0.6938 valid_Accuracy 0.5769 valid_f1 0.5924  test_Loss 0.6006 test_Accuracy 0.6923 test_f1 0.7057\n",
            "Loss 0.6384 train_f1 0.6088 Accuracy 0.6349   valid_Loss 0.6578 valid_Accuracy 0.6106 valid_f1 0.4765  test_Loss 0.5951 test_Accuracy 0.6442 test_f1 0.5462\n",
            "Loss 0.5950 train_f1 0.6449 Accuracy 0.6776   valid_Loss 0.9499 valid_Accuracy 0.5096 valid_f1 0.0488  test_Loss 0.7576 test_Accuracy 0.5288 test_f1 0.0774\n",
            "Loss 0.5348 train_f1 0.6509 Accuracy 0.7034   valid_Loss 0.9866 valid_Accuracy 0.6250 valid_f1 0.6484  test_Loss 0.5990 test_Accuracy 0.7356 test_f1 0.7526\n",
            "Loss 0.4534 train_f1 0.6981 Accuracy 0.7450   valid_Loss 0.7602 valid_Accuracy 0.6635 valid_f1 0.6887  test_Loss 0.4432 test_Accuracy 0.7885 test_f1 0.8043\n",
            "Loss 0.3876 train_f1 0.8067 Accuracy 0.8155   valid_Loss 1.1605 valid_Accuracy 0.6490 valid_f1 0.7017  test_Loss 0.6842 test_Accuracy 0.7163 test_f1 0.7618\n",
            "Loss 0.4551 train_f1 0.7590 Accuracy 0.7817   valid_Loss 0.9134 valid_Accuracy 0.6635 valid_f1 0.6709  test_Loss 0.5432 test_Accuracy 0.8077 test_f1 0.8166\n",
            "Loss 0.3295 train_f1 0.8286 Accuracy 0.8423   valid_Loss 0.9247 valid_Accuracy 0.6731 valid_f1 0.5498  test_Loss 0.6170 test_Accuracy 0.7115 test_f1 0.6375\n",
            "Loss 0.2331 train_f1 0.9017 Accuracy 0.9058   valid_Loss 1.4760 valid_Accuracy 0.6827 valid_f1 0.7188  test_Loss 0.8519 test_Accuracy 0.7596 test_f1 0.7859\n",
            "Loss 0.2900 train_f1 0.8442 Accuracy 0.8651   valid_Loss 1.5785 valid_Accuracy 0.6538 valid_f1 0.6592  test_Loss 0.7640 test_Accuracy 0.8221 test_f1 0.8123\n",
            "Loss 0.2092 train_f1 0.8991 Accuracy 0.9107   valid_Loss 2.2599 valid_Accuracy 0.5673 valid_f1 0.2654  test_Loss 1.9165 test_Accuracy 0.6010 test_f1 0.3417\n",
            "Loss 0.1727 train_f1 0.9225 Accuracy 0.9335   valid_Loss 1.4293 valid_Accuracy 0.6490 valid_f1 0.6890  test_Loss 0.8511 test_Accuracy 0.7788 test_f1 0.7724\n",
            "Loss 0.0947 train_f1 0.9679 Accuracy 0.9722   valid_Loss 1.2176 valid_Accuracy 0.7500 valid_f1 0.7572  test_Loss 0.9801 test_Accuracy 0.7837 test_f1 0.7695\n",
            "Loss 0.2013 train_f1 0.9122 Accuracy 0.9147   valid_Loss 1.2706 valid_Accuracy 0.7163 valid_f1 0.6797  test_Loss 1.0136 test_Accuracy 0.7692 test_f1 0.6642\n",
            "Loss 0.0665 train_f1 0.9710 Accuracy 0.9742   valid_Loss 1.4849 valid_Accuracy 0.7356 valid_f1 0.7256  test_Loss 1.0140 test_Accuracy 0.7981 test_f1 0.7769\n",
            "Loss 0.0527 train_f1 0.9759 Accuracy 0.9772   valid_Loss 1.9129 valid_Accuracy 0.7404 valid_f1 0.7543  test_Loss 1.1885 test_Accuracy 0.7885 test_f1 0.7923\n",
            "Loss 0.0430 train_f1 0.9791 Accuracy 0.9812   valid_Loss 2.0051 valid_Accuracy 0.7260 valid_f1 0.7322  test_Loss 1.2715 test_Accuracy 0.7885 test_f1 0.7874\n",
            "Loss 0.0334 train_f1 0.9927 Accuracy 0.9940   valid_Loss 1.9869 valid_Accuracy 0.7452 valid_f1 0.7341  test_Loss 1.3885 test_Accuracy 0.8029 test_f1 0.7915\n",
            "Loss 0.0088 train_f1 0.9991 Accuracy 0.9990   valid_Loss 1.8206 valid_Accuracy 0.7500 valid_f1 0.7353  test_Loss 1.4653 test_Accuracy 0.7548 test_f1 0.7319\n",
            "Loss 0.0493 train_f1 0.9830 Accuracy 0.9821   valid_Loss 1.8656 valid_Accuracy 0.7260 valid_f1 0.7400  test_Loss 1.4810 test_Accuracy 0.7692 test_f1 0.7738\n",
            "Loss 0.0068 train_f1 1.0000 Accuracy 1.0000   valid_Loss 2.0717 valid_Accuracy 0.7308 valid_f1 0.7238  test_Loss 1.4880 test_Accuracy 0.7981 test_f1 0.7956\n",
            "Loss 0.0193 train_f1 0.9924 Accuracy 0.9950   valid_Loss 1.8108 valid_Accuracy 0.7404 valid_f1 0.7340  test_Loss 1.7005 test_Accuracy 0.7452 test_f1 0.7281\n",
            "Loss 0.0249 train_f1 0.9917 Accuracy 0.9911   valid_Loss 2.0453 valid_Accuracy 0.7163 valid_f1 0.6869  test_Loss 1.6657 test_Accuracy 0.7500 test_f1 0.7387\n",
            "Loss 0.0233 train_f1 0.9877 Accuracy 0.9901   valid_Loss 2.1604 valid_Accuracy 0.7115 valid_f1 0.6965  test_Loss 1.6289 test_Accuracy 0.7740 test_f1 0.7434\n",
            "Loss 0.0074 train_f1 0.9981 Accuracy 0.9980   valid_Loss 2.4447 valid_Accuracy 0.7019 valid_f1 0.7114  test_Loss 2.1448 test_Accuracy 0.7596 test_f1 0.7811\n",
            "Loss 0.0048 train_f1 0.9992 Accuracy 0.9990   valid_Loss 2.0663 valid_Accuracy 0.7163 valid_f1 0.7008  test_Loss 1.7729 test_Accuracy 0.7404 test_f1 0.6915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save result"
      ],
      "metadata": {
        "id": "jVPyJN6nJxTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Train Accuracies': [i.numpy() for i in train_accuracies],\n",
        "    'Valid Accuracies': [i.numpy() for i in valid_accuracies],\n",
        "    'Test Accuracies': [i.numpy() for i in test_accuracies],\n",
        "    'Train F1 Scores': [i.numpy() for i in train_f1s],\n",
        "    'Valid F1 Scores': [i.numpy() for i in valid_f1s],\n",
        "    'Test F1 Scores': [i.numpy() for i in test_f1s]\n",
        "})\n",
        "\n",
        "excel_file_path = 'GRU_Disneyland5.xlsx'\n",
        "df.to_excel(excel_file_path, engine='openpyxl')\n",
        "\n",
        "!cp -r \"/content/GRU_Disneyland5.xlsx\" \"/content/drive/MyDrive/datasets/master_thesis/record/\""
      ],
      "metadata": {
        "id": "anqwptlD9k4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "highest_value = max(valid_accuracies)\n",
        "\n",
        "\n",
        "index_of_highest = valid_accuracies.index(highest_value)\n",
        "\n",
        "print(test_accuracies[index_of_highest], test_f1s[index_of_highest])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBgT-HdgC9Wa",
        "outputId": "fa0e733c-f3c6-4870-e1ba-2df699200db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.78365386, shape=(), dtype=float32) tf.Tensor(0.7694651, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PChxLeMJIhFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}